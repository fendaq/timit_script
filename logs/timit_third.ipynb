{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo\n",
    "- batchnorm\n",
    "- prepare batched data from full data set ( data[start:end] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mfccPath = os.path.join('data','train','feats')\n",
    "train_labelPath = os.path.join('data','train','labels')\n",
    "test_mfccPath = os.path.join('data','test','feats')\n",
    "test_labelPath = os.path.join('data','test','labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_batched_data(mfccPath, labelPath, batchSize):\n",
    "    ''' return 3-element tuple: batched data (list), maxTimeLength (int), total number of samples (int)\n",
    "        The shape of batched_data's elements is (batchSize, maxLength, nFeatures)\n",
    "    '''\n",
    "    \n",
    "    def list_to_sparse_tensor(targetList):\n",
    "        indices = []\n",
    "        vals = []\n",
    "\n",
    "        for tI, target in enumerate(targetList):\n",
    "            for seqI, val in enumerate(target):\n",
    "                indices.append([tI, seqI])\n",
    "                vals.append(val)\n",
    "        shape = [len(targetList), np.asarray(indices).max(axis=0)[1]+1]\n",
    "        return (np.array(indices), np.array(vals), np.array(shape))\n",
    "    \n",
    "    def data_lists_to_batches(inputList, targetList, batchSize):\n",
    "        nFeatures = inputList[0].shape[1]\n",
    "        maxLength = 0\n",
    "        for inp in inputList:\n",
    "            maxLength = max(maxLength, inp.shape[0])\n",
    "\n",
    "        randIdx = np.random.permutation(len(inputList))\n",
    "        start, end = (0, batchSize)\n",
    "        dataBatches = []\n",
    "\n",
    "        while end <= len(inputList):\n",
    "            batchSeqLengths = np.zeros(batchSize)\n",
    "\n",
    "            for batchI, origI in enumerate(randIdx[start:end]):\n",
    "                batchSeqLengths[batchI] = inputList[origI].shape[0]\n",
    "\n",
    "            batchInputs = np.zeros((batchSize, maxLength, nFeatures))\n",
    "            batchTargetList = []\n",
    "            for batchI, origI in enumerate(randIdx[start:end]):\n",
    "                padSecs = maxLength - inputList[origI].shape[0]\n",
    "                batchInputs[batchI,:,:] = np.pad(inputList[origI], ((0,padSecs),(0,0)), 'constant', constant_values=0)\n",
    "                batchTargetList.append(targetList[origI])\n",
    "            dataBatches.append((batchInputs, list_to_sparse_tensor(batchTargetList), batchSeqLengths))\n",
    "            start += batchSize\n",
    "            end += batchSize\n",
    "        return (dataBatches, maxLength)\n",
    "    \n",
    "    return data_lists_to_batches([np.load(os.path.join(mfccPath, fn)) for fn in os.listdir(mfccPath)],\n",
    "                                [np.load(os.path.join(labelPath, fn)) for fn in os.listdir(labelPath)],\n",
    "                                batchSize) + (len(os.listdir(mfccPath)),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchSize = 64\n",
    "num_features = 39\n",
    "num_hidden = 128\n",
    "num_classes = 39 + 1\n",
    "learning_rate = 0.001\n",
    "n_hidden_layer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batchedData, train_maxTimeSteps, train_totalN = load_batched_data(train_mfccPath, train_labelPath, batchSize)\n",
    "test_batchedData, test_maxTimeSteps, test_totalN = load_batched_data(test_mfccPath, test_labelPath, batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.float32, [None, None, num_features])\n",
    "targetIdx = tf.placeholder(tf.int64)\n",
    "targetVals = tf.placeholder(tf.int32)\n",
    "targetShape = tf.placeholder(tf.int64)\n",
    "targets = tf.SparseTensor(targetIdx, targetVals, targetShape)\n",
    "seq_len = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "cells_fw = [tf.nn.rnn_cell.LSTMCell(num_hidden)]\n",
    "cells_bw = [tf.nn.rnn_cell.LSTMCell(num_hidden)]\n",
    "outputs, output_state_fw, output_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(cells_fw, cells_bw, inputs,\n",
    "                                                                                           sequence_length=seq_len, dtype=tf.float32)\n",
    "\n",
    "shape = tf.shape(outputs)\n",
    "outputs = tf.reshape(outputs, [-1, shape[2]])\n",
    "W = tf.Variable(tf.truncated_normal([num_hidden*2, num_classes], stddev=0.1))\n",
    "b = tf.Variable(tf.constant(0., shape=[num_classes]))\n",
    "logits = tf.matmul(outputs, W) + b\n",
    "logits = tf.reshape(logits, [shape[0], shape[1], num_classes])\n",
    "logits = tf.transpose(logits, [1,0,2])\n",
    "\n",
    "loss = tf.nn.ctc_loss(labels=targets, inputs=logits, sequence_length=seq_len, time_major=True)\n",
    "cost = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(cost)\n",
    "decoded, log_prob = tf.nn.ctc_greedy_decoder(logits, seq_len)\n",
    "per = tf.reduce_sum(tf.edit_distance(tf.to_int32(decoded[0]), targets, normalize=False)) / tf.to_float(tf.size(targets.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, train_cost=128.559, train_per=0.911, test_cost=141.041, test_per=0.907, time = 72s\n",
      "Epoch 2/100, train_cost=133.743, train_per=0.848, test_cost=136.012, test_per=0.847, time = 73s\n",
      "Epoch 3/100, train_cost=131.283, train_per=0.846, test_cost=132.353, test_per=0.840, time = 73s\n",
      "Epoch 4/100, train_cost=132.713, train_per=0.838, test_cost=128.578, test_per=0.836, time = 73s\n",
      "Epoch 5/100, train_cost=133.845, train_per=0.847, test_cost=127.092, test_per=0.835, time = 74s\n",
      "Epoch 6/100, train_cost=125.500, train_per=0.831, test_cost=125.113, test_per=0.834, time = 73s\n",
      "Epoch 7/100, train_cost=124.064, train_per=0.832, test_cost=122.936, test_per=0.833, time = 73s\n",
      "Epoch 8/100, train_cost=120.641, train_per=0.818, test_cost=119.451, test_per=0.819, time = 73s\n",
      "Epoch 9/100, train_cost=101.984, train_per=0.837, test_cost=99.118, test_per=0.844, time = 73s\n",
      "Epoch 10/100, train_cost=79.823, train_per=0.816, test_cost=89.072, test_per=0.821, time = 73s\n",
      "Epoch 11/100, train_cost=84.132, train_per=0.790, test_cost=83.322, test_per=0.790, time = 73s\n",
      "Epoch 12/100, train_cost=82.224, train_per=0.767, test_cost=79.931, test_per=0.767, time = 73s\n",
      "Epoch 13/100, train_cost=75.125, train_per=0.734, test_cost=77.370, test_per=0.748, time = 73s\n",
      "Epoch 14/100, train_cost=76.463, train_per=0.725, test_cost=75.347, test_per=0.726, time = 73s\n",
      "Epoch 15/100, train_cost=74.184, train_per=0.701, test_cost=73.640, test_per=0.714, time = 73s\n",
      "Epoch 16/100, train_cost=73.096, train_per=0.692, test_cost=71.783, test_per=0.686, time = 73s\n",
      "Epoch 17/100, train_cost=61.205, train_per=0.663, test_cost=70.129, test_per=0.681, time = 73s\n",
      "Epoch 18/100, train_cost=70.324, train_per=0.641, test_cost=69.347, test_per=0.659, time = 74s\n",
      "Epoch 19/100, train_cost=69.419, train_per=0.655, test_cost=68.018, test_per=0.661, time = 73s\n",
      "Epoch 20/100, train_cost=67.237, train_per=0.626, test_cost=67.150, test_per=0.635, time = 73s\n",
      "Epoch 21/100, train_cost=66.250, train_per=0.612, test_cost=65.842, test_per=0.623, time = 74s\n",
      "Epoch 22/100, train_cost=64.312, train_per=0.604, test_cost=65.036, test_per=0.617, time = 73s\n",
      "Epoch 23/100, train_cost=58.047, train_per=0.572, test_cost=64.071, test_per=0.592, time = 73s\n",
      "Epoch 24/100, train_cost=61.342, train_per=0.592, test_cost=63.950, test_per=0.616, time = 73s\n",
      "Epoch 25/100, train_cost=58.499, train_per=0.575, test_cost=63.149, test_per=0.589, time = 73s\n",
      "Epoch 26/100, train_cost=61.720, train_per=0.574, test_cost=62.036, test_per=0.580, time = 73s\n",
      "Epoch 27/100, train_cost=55.395, train_per=0.533, test_cost=62.315, test_per=0.558, time = 73s\n",
      "Epoch 28/100, train_cost=59.754, train_per=0.533, test_cost=61.689, test_per=0.549, time = 73s\n",
      "Epoch 29/100, train_cost=60.744, train_per=0.579, test_cost=61.394, test_per=0.590, time = 73s\n",
      "Epoch 30/100, train_cost=57.939, train_per=0.530, test_cost=60.419, test_per=0.553, time = 73s\n",
      "Epoch 31/100, train_cost=56.038, train_per=0.556, test_cost=59.761, test_per=0.555, time = 73s\n",
      "Epoch 32/100, train_cost=55.866, train_per=0.527, test_cost=60.106, test_per=0.554, time = 73s\n",
      "Epoch 33/100, train_cost=56.821, train_per=0.524, test_cost=58.742, test_per=0.538, time = 73s\n",
      "Epoch 34/100, train_cost=57.448, train_per=0.522, test_cost=58.540, test_per=0.539, time = 73s\n",
      "Epoch 35/100, train_cost=52.108, train_per=0.522, test_cost=58.781, test_per=0.534, time = 73s\n",
      "Epoch 36/100, train_cost=54.281, train_per=0.485, test_cost=58.667, test_per=0.504, time = 73s\n",
      "Epoch 37/100, train_cost=54.208, train_per=0.493, test_cost=57.414, test_per=0.523, time = 73s\n",
      "Epoch 38/100, train_cost=52.499, train_per=0.499, test_cost=57.388, test_per=0.523, time = 73s\n",
      "Epoch 39/100, train_cost=53.043, train_per=0.462, test_cost=58.215, test_per=0.494, time = 74s\n",
      "Epoch 40/100, train_cost=53.618, train_per=0.476, test_cost=56.674, test_per=0.503, time = 73s\n",
      "Epoch 41/100, train_cost=52.513, train_per=0.489, test_cost=56.744, test_per=0.500, time = 73s\n",
      "Epoch 42/100, train_cost=54.607, train_per=0.489, test_cost=56.631, test_per=0.508, time = 73s\n",
      "Epoch 43/100, train_cost=53.910, train_per=0.504, test_cost=55.931, test_per=0.500, time = 74s\n",
      "Epoch 44/100, train_cost=52.627, train_per=0.480, test_cost=56.149, test_per=0.511, time = 73s\n",
      "Epoch 45/100, train_cost=45.181, train_per=0.445, test_cost=55.797, test_per=0.475, time = 73s\n",
      "Epoch 46/100, train_cost=52.388, train_per=0.469, test_cost=56.405, test_per=0.477, time = 73s\n",
      "Epoch 47/100, train_cost=48.996, train_per=0.466, test_cost=55.121, test_per=0.497, time = 73s\n",
      "Epoch 48/100, train_cost=48.121, train_per=0.448, test_cost=54.633, test_per=0.481, time = 73s\n",
      "Epoch 49/100, train_cost=48.592, train_per=0.442, test_cost=54.424, test_per=0.476, time = 73s\n",
      "Epoch 50/100, train_cost=47.325, train_per=0.456, test_cost=54.897, test_per=0.477, time = 73s\n",
      "Epoch 51/100, train_cost=50.039, train_per=0.457, test_cost=54.365, test_per=0.485, time = 73s\n",
      "Epoch 52/100, train_cost=47.870, train_per=0.436, test_cost=54.409, test_per=0.479, time = 73s\n",
      "Epoch 53/100, train_cost=49.146, train_per=0.424, test_cost=54.097, test_per=0.461, time = 73s\n",
      "Epoch 54/100, train_cost=47.698, train_per=0.426, test_cost=54.143, test_per=0.470, time = 73s\n",
      "Epoch 55/100, train_cost=50.986, train_per=0.446, test_cost=53.778, test_per=0.462, time = 73s\n",
      "Epoch 56/100, train_cost=51.813, train_per=0.448, test_cost=53.539, test_per=0.467, time = 73s\n",
      "Epoch 57/100, train_cost=46.720, train_per=0.415, test_cost=53.182, test_per=0.452, time = 73s\n",
      "Epoch 58/100, train_cost=43.802, train_per=0.433, test_cost=53.446, test_per=0.474, time = 73s\n",
      "Epoch 59/100, train_cost=45.745, train_per=0.415, test_cost=53.290, test_per=0.457, time = 73s\n",
      "Epoch 60/100, train_cost=48.357, train_per=0.407, test_cost=52.732, test_per=0.448, time = 73s\n",
      "Epoch 61/100, train_cost=44.972, train_per=0.430, test_cost=52.914, test_per=0.455, time = 73s\n",
      "Epoch 62/100, train_cost=49.852, train_per=0.399, test_cost=52.681, test_per=0.431, time = 73s\n",
      "Epoch 63/100, train_cost=46.678, train_per=0.406, test_cost=53.392, test_per=0.433, time = 73s\n",
      "Epoch 64/100, train_cost=47.014, train_per=0.390, test_cost=52.674, test_per=0.440, time = 73s\n",
      "Epoch 65/100, train_cost=48.595, train_per=0.412, test_cost=52.568, test_per=0.446, time = 73s\n",
      "Epoch 66/100, train_cost=47.189, train_per=0.412, test_cost=52.080, test_per=0.452, time = 73s\n",
      "Epoch 67/100, train_cost=46.337, train_per=0.418, test_cost=52.044, test_per=0.455, time = 73s\n",
      "Epoch 68/100, train_cost=46.855, train_per=0.392, test_cost=51.869, test_per=0.436, time = 73s\n",
      "Epoch 69/100, train_cost=44.583, train_per=0.376, test_cost=52.005, test_per=0.433, time = 73s\n",
      "Epoch 70/100, train_cost=44.873, train_per=0.402, test_cost=51.635, test_per=0.434, time = 73s\n",
      "Epoch 71/100, train_cost=48.022, train_per=0.414, test_cost=51.652, test_per=0.438, time = 73s\n",
      "Epoch 72/100, train_cost=47.877, train_per=0.404, test_cost=51.553, test_per=0.424, time = 73s\n",
      "Epoch 73/100, train_cost=46.294, train_per=0.382, test_cost=51.367, test_per=0.423, time = 73s\n",
      "Epoch 74/100, train_cost=47.594, train_per=0.389, test_cost=51.594, test_per=0.420, time = 73s\n",
      "Epoch 75/100, train_cost=39.969, train_per=0.382, test_cost=51.752, test_per=0.430, time = 73s\n",
      "Epoch 76/100, train_cost=41.816, train_per=0.394, test_cost=51.103, test_per=0.425, time = 73s\n",
      "Epoch 77/100, train_cost=41.311, train_per=0.388, test_cost=51.543, test_per=0.435, time = 73s\n",
      "Epoch 78/100, train_cost=39.760, train_per=0.392, test_cost=51.730, test_per=0.445, time = 73s\n",
      "Epoch 79/100, train_cost=43.212, train_per=0.376, test_cost=50.768, test_per=0.418, time = 73s\n",
      "Epoch 80/100, train_cost=43.130, train_per=0.345, test_cost=50.855, test_per=0.411, time = 73s\n",
      "Epoch 81/100, train_cost=42.425, train_per=0.370, test_cost=50.900, test_per=0.428, time = 73s\n",
      "Epoch 82/100, train_cost=44.892, train_per=0.367, test_cost=51.342, test_per=0.406, time = 73s\n",
      "Epoch 83/100, train_cost=43.008, train_per=0.357, test_cost=50.345, test_per=0.417, time = 73s\n",
      "Epoch 84/100, train_cost=42.042, train_per=0.358, test_cost=50.372, test_per=0.402, time = 73s\n",
      "Epoch 85/100, train_cost=40.397, train_per=0.362, test_cost=50.857, test_per=0.422, time = 73s\n",
      "Epoch 86/100, train_cost=42.767, train_per=0.391, test_cost=51.177, test_per=0.415, time = 74s\n",
      "Epoch 87/100, train_cost=41.575, train_per=0.330, test_cost=50.611, test_per=0.400, time = 73s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100, train_cost=40.191, train_per=0.358, test_cost=50.316, test_per=0.412, time = 73s\n",
      "Epoch 89/100, train_cost=38.882, train_per=0.374, test_cost=50.708, test_per=0.419, time = 73s\n",
      "Epoch 90/100, train_cost=39.657, train_per=0.351, test_cost=50.122, test_per=0.413, time = 73s\n",
      "Epoch 91/100, train_cost=41.207, train_per=0.356, test_cost=50.179, test_per=0.410, time = 73s\n",
      "Epoch 92/100, train_cost=40.734, train_per=0.347, test_cost=49.933, test_per=0.408, time = 74s\n",
      "Epoch 93/100, train_cost=42.481, train_per=0.367, test_cost=50.317, test_per=0.419, time = 73s\n",
      "Epoch 94/100, train_cost=45.343, train_per=0.364, test_cost=50.749, test_per=0.400, time = 73s\n",
      "Epoch 95/100, train_cost=44.174, train_per=0.365, test_cost=49.650, test_per=0.398, time = 73s\n",
      "Epoch 96/100, train_cost=42.320, train_per=0.351, test_cost=49.433, test_per=0.401, time = 73s\n",
      "Epoch 97/100, train_cost=42.693, train_per=0.365, test_cost=49.609, test_per=0.399, time = 73s\n",
      "Epoch 98/100, train_cost=36.640, train_per=0.330, test_cost=49.916, test_per=0.410, time = 73s\n",
      "Epoch 99/100, train_cost=42.195, train_per=0.353, test_cost=49.357, test_per=0.405, time = 73s\n",
      "Epoch 100/100, train_cost=41.459, train_per=0.344, test_cost=49.367, test_per=0.395, time = 73s\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        batchRandIdx = np.random.permutation(len(train_batchedData))\n",
    "        \n",
    "        train_cost = train_per = 0\n",
    "        for batch, batchOrgI in enumerate(batchRandIdx):\n",
    "            batchInputs, batchTargetSparse, batchSeqLengths = train_batchedData[batchOrgI]\n",
    "            batchTargetIdx, batchTargetVals, batchTargetShape = batchTargetSparse\n",
    "            feedDict = {inputs: batchInputs, targetIdx: batchTargetIdx, targetVals: batchTargetVals,\n",
    "                        targetShape: batchTargetShape, seq_len: batchSeqLengths}\n",
    "            optimizer.run(feed_dict=feedDict)\n",
    "            \n",
    "            if batch == len(batchRandIdx) - 1:\n",
    "                train_cost, train_per = sess.run([cost, per], feedDict)\n",
    "            \n",
    "        test_cost = test_per = 0\n",
    "        for i in range(len(test_batchedData)):\n",
    "            batchInputs, batchTargetSparse, batchSeqLengths = test_batchedData[i]\n",
    "            batchTargetIdx, batchTargetVals, batchTargetShape = batchTargetSparse\n",
    "            feedDict = {inputs: batchInputs, targetIdx: batchTargetIdx, targetVals: batchTargetVals,\n",
    "                        targetShape: batchTargetShape, seq_len: batchSeqLengths}\n",
    "            \n",
    "            batch_cost, batch_per = sess.run([cost, per], feedDict)\n",
    "            test_cost += batch_cost\n",
    "            test_per += batch_per\n",
    "        \n",
    "        test_cost /= len(test_batchedData)\n",
    "        test_per /= len(test_batchedData)\n",
    "        \n",
    "        end = time.time()\n",
    "        log = \"Epoch {}/{}, train_cost={:.3f}, train_per={:.3f}, test_cost={:.3f}, test_per={:.3f}, time = {:.0f}s\"\n",
    "        print(log.format(epoch+1, epochs, train_cost, train_per, test_cost, test_per, end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
