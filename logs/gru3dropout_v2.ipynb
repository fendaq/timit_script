{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connectionist Temporal Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phn_61 = ['aa', 'ae', 'ah', 'ao', 'aw', 'ax', 'ax-h', 'axr', 'ay', 'b', 'bcl', 'ch', 'd', 'dcl', 'dh', 'dx', 'eh', 'el', 'em', 'en', 'eng', 'epi', 'er', 'ey', 'f', 'g', 'gcl', 'h#', 'hh', 'hv', 'ih', 'ix', 'iy', 'jh', 'k', 'kcl', 'l', 'm', 'n', 'ng', 'nx', 'ow', 'oy', 'p', 'pau', 'pcl', 'q', 'r', 's', 'sh', 't', 'tcl', 'th', 'uh', 'uw', 'ux', 'v', 'w', 'y', 'z', 'zh']\n",
    "phn_39 = ['ae', 'ao', 'aw', 'ax', 'ay', 'b', 'ch', 'd', 'dh', 'dx', 'eh', 'er', 'ey', 'f', 'g', 'h#', 'hh', 'ix', 'iy', 'jh', 'k', 'l', 'm', 'n', 'ng', 'ow', 'oy', 'p', 'r', 's', 't', 'th', 'uh', 'uw', 'v', 'w', 'y', 'z', 'zh']\n",
    "mapping = {'ah': 'ax', 'ax-h': 'ax', 'ux': 'uw', 'aa': 'ao', 'ih': 'ix', 'axr': 'er', 'el': 'l', 'em': 'm', 'en': 'n', 'nx': 'n', 'eng': 'ng', 'sh': 'zh', 'hv': 'hh', 'bcl': 'h#', 'pcl': 'h#', 'dcl': 'h#', 'tcl': 'h#', 'gcl': 'h#', 'kcl': 'h#', 'q': 'h#', 'epi': 'h#', 'pau': 'h#'}\n",
    "\n",
    "train_featPath = os.path.join('data','train','feats','mfcc')\n",
    "train_labelPath = os.path.join('data','train','labels')\n",
    "test_featPath = os.path.join('data','test','feats','mfcc')\n",
    "test_labelPath = os.path.join('data','test','labels')\n",
    "\n",
    "feat_type = 'mfcc'\n",
    "batchSize = 128\n",
    "feats_dim = 39 if feat_type=='mfcc' else 123 # filter bank\n",
    "num_hidden = 128\n",
    "num_classes = len(phn_61)+1 # num of phoneme + blank\n",
    "learning_rate = 0.001\n",
    "n_hidden_layer = 3\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_batched_data(featPath, labelPath, batchSize):\n",
    "    ''' return 3-element tuple: batched data (list), maxTimeLengths (list), total number of samples (int)\n",
    "        The shape of batched_data's elements is (batchSize, maxLength, nFeatures)\n",
    "    '''\n",
    "    def list_to_sparse_tensor(targetList):\n",
    "        indices = []\n",
    "        vals = []\n",
    "\n",
    "        for tI, target in enumerate(targetList):\n",
    "            for seqI, val in enumerate(target):\n",
    "                indices.append([tI, seqI])\n",
    "                vals.append(val)\n",
    "        shape = [len(targetList), np.asarray(indices).max(axis=0)[1]+1]\n",
    "        return (np.array(indices), np.array(vals), np.array(shape))\n",
    "    \n",
    "    def data_lists_to_batches(inputList, targetList, batchSize):\n",
    "        nFeatures = inputList[0].shape[1]\n",
    "        input_target = list(zip(inputList, targetList))\n",
    "        input_target.sort(key=lambda x: x[0].shape[0])\n",
    "        inputList = []\n",
    "        targetList = []\n",
    "        for inp, tar in input_target:\n",
    "            inputList.append(inp)\n",
    "            targetList.append(tar)\n",
    "    \n",
    "        start, end = (0, batchSize)\n",
    "        dataBatches = []\n",
    "        maxLengths = []\n",
    "\n",
    "        while end <= len(inputList):\n",
    "            batchSeqLengths = np.zeros(batchSize)\n",
    "                \n",
    "            maxLength = 0\n",
    "            for batchI, i in enumerate(range(start, end)):\n",
    "                batchSeqLengths[batchI] = inputList[i].shape[0]\n",
    "                maxLength = max(maxLength, inputList[i].shape[0])\n",
    "            maxLengths.append(maxLength)\n",
    "\n",
    "            batchInputs = np.zeros((batchSize, maxLength, nFeatures))\n",
    "            batchTargetList = []\n",
    "            for batchI, i in enumerate(range(start, end)):\n",
    "                padSecs = maxLength - inputList[i].shape[0]\n",
    "                batchInputs[batchI,:,:] = np.pad(inputList[i], ((0,padSecs),(0,0)), 'constant', constant_values=0)\n",
    "                batchTargetList.append(targetList[i])\n",
    "            dataBatches.append((batchInputs, list_to_sparse_tensor(batchTargetList), batchSeqLengths))\n",
    "            start += batchSize\n",
    "            end += batchSize\n",
    "            \n",
    "            if end > len(inputList) and start < len(inputList):\n",
    "                end = len(inputList)\n",
    "                batchSize = end - start\n",
    "        return (dataBatches, maxLengths)\n",
    "    \n",
    "    return data_lists_to_batches([np.load(os.path.join(featPath, fn)) for fn in os.listdir(featPath)],\n",
    "                                [np.load(os.path.join(labelPath, fn)) for fn in os.listdir(labelPath)],\n",
    "                                batchSize) + (len(os.listdir(featPath)),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batchedData, train_maxTimeLengths, train_totalN = load_batched_data(train_featPath, train_labelPath, batchSize)\n",
    "test_batchedData, test_maxTimeLengths, test_totalN = load_batched_data(test_featPath, test_labelPath, batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "phn_61_tensor = tf.constant(phn_61, dtype=tf.string)\n",
    "phn_39_tensor = tf.constant(phn_39, dtype=tf.string)\n",
    "mapping_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(list(mapping.keys()), list(mapping.values())), default_value='')\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [None, None, feats_dim]) # shape=[batch, max_time, features]\n",
    "targets = tf.sparse_placeholder(tf.int32)\n",
    "seq_len = tf.placeholder(tf.int32, [None])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "cells_fw = [tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.GRUCell(num_hidden), output_keep_prob=keep_prob) for _ in range(n_hidden_layer)]\n",
    "cells_bw = [tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.GRUCell(num_hidden), output_keep_prob=keep_prob) for _ in range(n_hidden_layer)]\n",
    "outputs, _, _ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(cells_fw, cells_bw, inputs, dtype=tf.float32, sequence_length=seq_len)\n",
    "\n",
    "shape = tf.shape(outputs)\n",
    "outputs = tf.reshape(outputs, [-1, shape[2]])\n",
    "W = tf.Variable(tf.truncated_normal([num_hidden*2, num_classes], stddev=0.1))\n",
    "b = tf.Variable(tf.constant(0., shape=[num_classes]))\n",
    "logits = tf.matmul(outputs, W) + b\n",
    "logits = tf.reshape(logits, [shape[0], shape[1], num_classes])\n",
    "logits = tf.transpose(logits, [1,0,2]) # time major shape\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.ctc_loss(labels=targets, inputs=logits, sequence_length=seq_len, time_major=True))\n",
    "params = tf.trainable_variables()\n",
    "gradients = tf.gradients(loss, params)\n",
    "clipped_gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "update_step = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "decoded, _ = tf.nn.ctc_greedy_decoder(logits, seq_len)\n",
    "decoded = tf.to_int32(decoded[0])\n",
    "\n",
    "def map_to_reduced_phn(p):\n",
    "    val = mapping_table.lookup(phn_61_tensor[p])\n",
    "    f1 = lambda: tf.to_int32(tf.reduce_min(tf.where(tf.equal(val, phn_39_tensor))))\n",
    "    f2 = lambda: tf.to_int32(tf.reduce_min(tf.where(tf.equal(phn_61_tensor[p], phn_39_tensor))))\n",
    "    return tf.cond(tf.not_equal(val, ''), f1, f2)\n",
    "        \n",
    "decoded_reduced = tf.SparseTensor(decoded.indices, tf.map_fn(map_to_reduced_phn, decoded.values), decoded.dense_shape)\n",
    "targets_reduced = tf.SparseTensor(targets.indices, tf.map_fn(map_to_reduced_phn, targets.values), targets.dense_shape)\n",
    "per = tf.reduce_sum(tf.edit_distance(decoded_reduced, targets_reduced, normalize=False)) / tf.to_float(tf.size(targets_reduced.values))\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, train_cost=135.213, test_cost=154.960, time = 53s\n",
      "Epoch 2/50, train_cost=99.441, test_cost=124.134, time = 53s\n",
      "Epoch 3/50, train_cost=82.660, test_cost=104.870, time = 54s\n",
      "Epoch 4/50, train_cost=76.198, test_cost=94.064, time = 55s\n",
      "Epoch 5/50, train_cost=128.602, test_cost=85.950, time = 55s\n",
      "Epoch 6/50, train_cost=56.312, test_cost=79.640, time = 54s\n",
      "Epoch 7/50, train_cost=91.204, test_cost=75.002, time = 55s\n",
      "Epoch 8/50, train_cost=107.524, test_cost=72.128, time = 55s\n",
      "Epoch 9/50, train_cost=40.024, test_cost=68.252, time = 55s\n",
      "Epoch 10/50, train_cost=71.514, test_cost=66.389, time = 54s\n",
      "Epoch 11/50, train_cost=67.001, test_cost=62.702, time = 53s\n",
      "Epoch 12/50, train_cost=58.535, test_cost=60.919, time = 53s\n",
      "Epoch 13/50, train_cost=41.886, test_cost=57.760, time = 53s\n",
      "Epoch 14/50, train_cost=47.788, test_cost=55.223, time = 53s\n",
      "Epoch 15/50, train_cost=49.230, test_cost=53.121, time = 53s\n",
      "Epoch 16/50, train_cost=67.848, test_cost=51.610, time = 53s\n",
      "Epoch 17/50, train_cost=22.148, test_cost=50.458, time = 53s\n",
      "Epoch 18/50, train_cost=56.902, test_cost=48.858, time = 53s\n",
      "Epoch 19/50, train_cost=57.983, test_cost=48.318, time = 53s\n",
      "Epoch 20/50, train_cost=62.983, test_cost=46.990, time = 54s\n",
      "Epoch 21/50, train_cost=28.822, test_cost=46.576, time = 53s\n",
      "Epoch 22/50, train_cost=24.401, test_cost=46.076, time = 53s\n",
      "Epoch 23/50, train_cost=31.055, test_cost=45.321, time = 53s\n",
      "Epoch 24/50, train_cost=48.456, test_cost=44.777, time = 53s\n",
      "Epoch 25/50, train_cost=12.902, test_cost=44.362, time = 53s\n",
      "Epoch 26/50, train_cost=24.566, test_cost=43.994, time = 53s\n",
      "Epoch 27/50, train_cost=25.534, test_cost=43.855, time = 53s\n",
      "Epoch 28/50, train_cost=23.277, test_cost=43.605, time = 53s\n",
      "Epoch 29/50, train_cost=23.538, test_cost=43.865, time = 53s\n",
      "Epoch 30/50, train_cost=22.386, test_cost=43.692, time = 53s\n",
      "Epoch 31/50, train_cost=39.359, test_cost=43.075, time = 54s\n",
      "Epoch 32/50, train_cost=48.131, test_cost=42.899, time = 54s\n",
      "Epoch 33/50, train_cost=27.395, test_cost=42.520, time = 54s\n",
      "Epoch 34/50, train_cost=30.258, test_cost=42.844, time = 54s\n",
      "Epoch 35/50, train_cost=21.010, test_cost=42.500, time = 54s\n",
      "Epoch 36/50, train_cost=39.230, test_cost=42.741, time = 55s\n",
      "Epoch 37/50, train_cost=33.344, test_cost=42.392, time = 54s\n",
      "Epoch 38/50, train_cost=20.553, test_cost=42.028, time = 54s\n",
      "Epoch 39/50, train_cost=19.731, test_cost=42.380, time = 53s\n",
      "Epoch 40/50, train_cost=39.818, test_cost=42.794, time = 54s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-8300758a7123>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mbatchInputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchTargetSparse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchSeqLengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_batchedData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatchOrgI\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mfeedDict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatchInputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatchTargetSparse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatchSeqLengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.6\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mupdate_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeedDict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchRandIdx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m   1698\u001b[0m         \u001b[0mnone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1699\u001b[0m     \"\"\"\n\u001b[1;32m-> 1700\u001b[1;33m     \u001b[0m_run_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[1;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   4071\u001b[0m                        \u001b[1;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4072\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 4073\u001b[1;33m   \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(mapping_table.init)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        batchRandIdx = np.random.permutation(len(train_batchedData))\n",
    "        \n",
    "        for batch, batchOrgI in enumerate(batchRandIdx):\n",
    "            batchInputs, batchTargetSparse, batchSeqLengths = train_batchedData[batchOrgI]\n",
    "            feedDict = {inputs: batchInputs, targets: batchTargetSparse, seq_len: batchSeqLengths, keep_prob: 0.6}\n",
    "            update_step.run(feed_dict=feedDict)\n",
    "            \n",
    "            if batch == len(batchRandIdx) - 1:\n",
    "                feedDict[keep_prob] = 1.0\n",
    "                train_cost = sess.run(loss, feedDict)\n",
    "            \n",
    "        test_cost = test_per = 0\n",
    "        for i in range(len(test_batchedData)):\n",
    "            batchInputs, batchTargetSparse, batchSeqLengths = test_batchedData[i]\n",
    "            feedDict = {inputs: batchInputs, targets: batchTargetSparse, seq_len: batchSeqLengths, keep_prob:1.0}\n",
    "            \n",
    "            batch_cost = sess.run(loss, feedDict)\n",
    "            test_cost += batch_cost\n",
    "        \n",
    "        test_cost /= len(test_batchedData)\n",
    "        \n",
    "        end = time.time()\n",
    "        log = \"Epoch {}/{}, train_cost={:.3f}, test_cost={:.3f}, time = {:.0f}s\"\n",
    "        print(log.format(epoch+1, epochs, train_cost, test_cost, end-start))\n",
    "        \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            saver.save(sess, 'model/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model.ckpt\n",
      "Epoch 41/50, train_cost=24.743, train_per=0.164, test_cost=42.794, test_per=0.260, time = 213s\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, 'model/model.ckpt')\n",
    "    sess.run(mapping_table.init)\n",
    "    \n",
    "    start = time.time()\n",
    "    train_cost = train_per = 0\n",
    "    for i in range(len(train_batchedData)):\n",
    "            batchInputs, batchTargetSparse, batchSeqLengths = train_batchedData[i]\n",
    "            feedDict = {inputs: batchInputs, targets: batchTargetSparse, seq_len: batchSeqLengths, keep_prob:1.0}\n",
    "            batch_cost, batch_per = sess.run([loss, per], feedDict)\n",
    "            train_cost += batch_cost\n",
    "            train_per += batch_per\n",
    "    \n",
    "    test_cost = test_per = 0\n",
    "    for i in range(len(test_batchedData)):\n",
    "            batchInputs, batchTargetSparse, batchSeqLengths = test_batchedData[i]\n",
    "            feedDict = {inputs: batchInputs, targets: batchTargetSparse, seq_len: batchSeqLengths, keep_prob:1.0}\n",
    "            batch_cost, batch_per = sess.run([loss, per], feedDict)\n",
    "            test_cost += batch_cost\n",
    "            test_per += batch_per\n",
    "            \n",
    "    train_cost /= len(train_batchedData)\n",
    "    train_per /= len(train_batchedData)\n",
    "    test_cost /= len(test_batchedData)\n",
    "    test_per /= len(test_batchedData)\n",
    "    \n",
    "    end = time.time()\n",
    "    log = \"Epoch {}/{}, train_cost={:.3f}, train_per={:.3f}, test_cost={:.3f}, test_per={:.3f}, time = {:.0f}s\"\n",
    "    print(log.format(epoch+1, epochs, train_cost, train_per, test_cost, test_per, end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
