{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo\n",
    "- batchnorm\n",
    "- prepare batched data from full data set ( data[start:end] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mfccPath = os.path.join('data','train','feats')\n",
    "train_labelPath = os.path.join('data','train','labels')\n",
    "test_mfccPath = os.path.join('data','test','feats')\n",
    "test_labelPath = os.path.join('data','test','labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_batched_data(mfccPath, labelPath, batchSize):\n",
    "    ''' return 3-element tuple: batched data (list), maxTimeLength (int), total number of samples (int)\n",
    "        The shape of batched_data's elements is (batchSize, maxLength, nFeatures)\n",
    "    '''\n",
    "    \n",
    "    def list_to_sparse_tensor(targetList):\n",
    "        indices = []\n",
    "        vals = []\n",
    "\n",
    "        for tI, target in enumerate(targetList):\n",
    "            for seqI, val in enumerate(target):\n",
    "                indices.append([tI, seqI])\n",
    "                vals.append(val)\n",
    "        shape = [len(targetList), np.asarray(indices).max(axis=0)[1]+1]\n",
    "        return (np.array(indices), np.array(vals), np.array(shape))\n",
    "    \n",
    "    def data_lists_to_batches(inputList, targetList, batchSize):\n",
    "        nFeatures = inputList[0].shape[1]\n",
    "        maxLength = 0\n",
    "        for inp in inputList:\n",
    "            maxLength = max(maxLength, inp.shape[0])\n",
    "\n",
    "        randIdx = np.random.permutation(len(inputList))\n",
    "        start, end = (0, batchSize)\n",
    "        dataBatches = []\n",
    "\n",
    "        while end <= len(inputList):\n",
    "            batchSeqLengths = np.zeros(batchSize)\n",
    "\n",
    "            for batchI, origI in enumerate(randIdx[start:end]):\n",
    "                batchSeqLengths[batchI] = inputList[origI].shape[0]\n",
    "\n",
    "            batchInputs = np.zeros((batchSize, maxLength, nFeatures))\n",
    "            batchTargetList = []\n",
    "            for batchI, origI in enumerate(randIdx[start:end]):\n",
    "                padSecs = maxLength - inputList[origI].shape[0]\n",
    "                batchInputs[batchI,:,:] = np.pad(inputList[origI], ((0,padSecs),(0,0)), 'constant', constant_values=0)\n",
    "                batchTargetList.append(targetList[origI])\n",
    "            dataBatches.append((batchInputs, list_to_sparse_tensor(batchTargetList), batchSeqLengths))\n",
    "            start += batchSize\n",
    "            end += batchSize\n",
    "        return (dataBatches, maxLength)\n",
    "    \n",
    "    return data_lists_to_batches([np.load(os.path.join(mfccPath, fn)) for fn in os.listdir(mfccPath)],\n",
    "                                [np.load(os.path.join(labelPath, fn)) for fn in os.listdir(labelPath)],\n",
    "                                batchSize) + (len(os.listdir(mfccPath)),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchSize = 128\n",
    "num_features = 39\n",
    "num_hidden = 128\n",
    "num_classes = 39 + 1\n",
    "learning_rate = 0.001\n",
    "n_hidden_layer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batchedData, train_maxTimeSteps, train_totalN = load_batched_data(train_mfccPath, train_labelPath, batchSize)\n",
    "test_batchedData, test_maxTimeSteps, test_totalN = load_batched_data(test_mfccPath, test_labelPath, batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.float32, [None, None, num_features])\n",
    "targetIdx = tf.placeholder(tf.int64)\n",
    "targetVals = tf.placeholder(tf.int32)\n",
    "targetShape = tf.placeholder(tf.int64)\n",
    "targets = tf.SparseTensor(targetIdx, targetVals, targetShape)\n",
    "seq_len = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "cells_fw = [tf.nn.rnn_cell.LSTMCell(num_hidden)]\n",
    "cells_bw = [tf.nn.rnn_cell.LSTMCell(num_hidden)]\n",
    "outputs, output_state_fw, output_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(cells_fw, cells_bw, inputs,\n",
    "                                                                                           sequence_length=seq_len, dtype=tf.float32)\n",
    "\n",
    "shape = tf.shape(outputs)\n",
    "outputs = tf.reshape(outputs, [-1, shape[2]])\n",
    "W = tf.Variable(tf.truncated_normal([num_hidden*2, num_classes], stddev=0.1))\n",
    "b = tf.Variable(tf.constant(0., shape=[num_classes]))\n",
    "logits = tf.matmul(outputs, W) + b\n",
    "logits = tf.reshape(logits, [shape[0], shape[1], num_classes])\n",
    "logits = tf.transpose(logits, [1,0,2])\n",
    "\n",
    "loss = tf.nn.ctc_loss(labels=targets, inputs=logits, sequence_length=seq_len, time_major=True)\n",
    "cost = tf.reduce_mean(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "decoded, log_prob = tf.nn.ctc_greedy_decoder(logits, seq_len)\n",
    "per = tf.reduce_sum(tf.edit_distance(tf.to_int32(decoded[0]), targets, normalize=False)) / tf.to_float(tf.size(targets.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120, train_cost=162.030, train_per=0.961, test_cost=162.989, test_per=0.960, time = 39s\n",
      "Epoch 2/120, train_cost=148.483, train_per=0.923, test_cost=146.534, test_per=0.930, time = 39s\n",
      "Epoch 3/120, train_cost=140.089, train_per=0.884, test_cost=140.830, test_per=0.893, time = 39s\n",
      "Epoch 4/120, train_cost=134.948, train_per=0.872, test_cost=138.217, test_per=0.879, time = 39s\n",
      "Epoch 5/120, train_cost=136.085, train_per=0.866, test_cost=135.837, test_per=0.869, time = 39s\n",
      "Epoch 6/120, train_cost=133.789, train_per=0.864, test_cost=133.716, test_per=0.863, time = 39s\n",
      "Epoch 7/120, train_cost=123.811, train_per=0.853, test_cost=131.713, test_per=0.856, time = 39s\n",
      "Epoch 8/120, train_cost=124.632, train_per=0.845, test_cost=129.825, test_per=0.851, time = 40s\n",
      "Epoch 9/120, train_cost=127.003, train_per=0.852, test_cost=128.454, test_per=0.852, time = 39s\n",
      "Epoch 10/120, train_cost=124.944, train_per=0.846, test_cost=127.423, test_per=0.845, time = 39s\n",
      "Epoch 11/120, train_cost=123.485, train_per=0.841, test_cost=126.304, test_per=0.846, time = 39s\n",
      "Epoch 12/120, train_cost=122.549, train_per=0.839, test_cost=125.415, test_per=0.845, time = 39s\n",
      "Epoch 13/120, train_cost=115.371, train_per=0.841, test_cost=124.539, test_per=0.847, time = 39s\n",
      "Epoch 14/120, train_cost=116.854, train_per=0.832, test_cost=123.777, test_per=0.839, time = 39s\n",
      "Epoch 15/120, train_cost=116.221, train_per=0.825, test_cost=123.012, test_per=0.833, time = 39s\n",
      "Epoch 16/120, train_cost=117.508, train_per=0.824, test_cost=122.476, test_per=0.832, time = 39s\n",
      "Epoch 17/120, train_cost=112.970, train_per=0.822, test_cost=121.756, test_per=0.822, time = 40s\n",
      "Epoch 18/120, train_cost=116.956, train_per=0.821, test_cost=120.948, test_per=0.825, time = 40s\n",
      "Epoch 19/120, train_cost=111.817, train_per=0.810, test_cost=120.008, test_per=0.819, time = 39s\n",
      "Epoch 20/120, train_cost=110.031, train_per=0.817, test_cost=119.487, test_per=0.819, time = 39s\n",
      "Epoch 21/120, train_cost=110.601, train_per=0.799, test_cost=118.747, test_per=0.807, time = 39s\n",
      "Epoch 22/120, train_cost=115.981, train_per=0.799, test_cost=117.985, test_per=0.807, time = 39s\n",
      "Epoch 23/120, train_cost=110.888, train_per=0.783, test_cost=117.340, test_per=0.796, time = 39s\n",
      "Epoch 24/120, train_cost=104.468, train_per=0.790, test_cost=116.428, test_per=0.795, time = 39s\n",
      "Epoch 25/120, train_cost=109.073, train_per=0.788, test_cost=115.979, test_per=0.793, time = 39s\n",
      "Epoch 26/120, train_cost=111.232, train_per=0.782, test_cost=115.477, test_per=0.786, time = 39s\n",
      "Epoch 27/120, train_cost=106.636, train_per=0.773, test_cost=114.491, test_per=0.784, time = 39s\n",
      "Epoch 28/120, train_cost=108.103, train_per=0.779, test_cost=112.860, test_per=0.788, time = 39s\n",
      "Epoch 29/120, train_cost=106.070, train_per=0.799, test_cost=110.973, test_per=0.794, time = 39s\n",
      "Epoch 30/120, train_cost=109.482, train_per=0.797, test_cost=114.095, test_per=0.795, time = 39s\n",
      "Epoch 31/120, train_cost=96.510, train_per=0.829, test_cost=96.850, test_per=0.827, time = 40s\n",
      "Epoch 32/120, train_cost=80.508, train_per=0.788, test_cost=83.640, test_per=0.789, time = 39s\n",
      "Epoch 33/120, train_cost=78.274, train_per=0.734, test_cost=78.226, test_per=0.750, time = 39s\n",
      "Epoch 34/120, train_cost=72.533, train_per=0.696, test_cost=74.785, test_per=0.713, time = 39s\n",
      "Epoch 35/120, train_cost=72.360, train_per=0.687, test_cost=72.781, test_per=0.697, time = 39s\n",
      "Epoch 36/120, train_cost=65.898, train_per=0.667, test_cost=70.297, test_per=0.680, time = 39s\n",
      "Epoch 37/120, train_cost=66.235, train_per=0.620, test_cost=67.742, test_per=0.644, time = 39s\n",
      "Epoch 38/120, train_cost=64.675, train_per=0.612, test_cost=65.674, test_per=0.623, time = 39s\n",
      "Epoch 39/120, train_cost=62.226, train_per=0.594, test_cost=63.973, test_per=0.605, time = 39s\n",
      "Epoch 40/120, train_cost=60.405, train_per=0.554, test_cost=62.464, test_per=0.587, time = 39s\n",
      "Epoch 41/120, train_cost=58.425, train_per=0.574, test_cost=61.158, test_per=0.584, time = 39s\n",
      "Epoch 42/120, train_cost=56.262, train_per=0.532, test_cost=59.920, test_per=0.560, time = 39s\n",
      "Epoch 43/120, train_cost=54.572, train_per=0.537, test_cost=58.859, test_per=0.548, time = 39s\n",
      "Epoch 44/120, train_cost=55.827, train_per=0.525, test_cost=57.830, test_per=0.538, time = 40s\n",
      "Epoch 45/120, train_cost=54.206, train_per=0.486, test_cost=56.904, test_per=0.518, time = 39s\n",
      "Epoch 46/120, train_cost=49.730, train_per=0.473, test_cost=56.073, test_per=0.515, time = 39s\n",
      "Epoch 47/120, train_cost=51.985, train_per=0.475, test_cost=55.291, test_per=0.509, time = 39s\n",
      "Epoch 48/120, train_cost=47.817, train_per=0.455, test_cost=54.587, test_per=0.488, time = 39s\n",
      "Epoch 49/120, train_cost=49.496, train_per=0.448, test_cost=53.888, test_per=0.481, time = 39s\n",
      "Epoch 50/120, train_cost=48.435, train_per=0.442, test_cost=53.094, test_per=0.476, time = 39s\n",
      "Epoch 51/120, train_cost=48.061, train_per=0.439, test_cost=52.530, test_per=0.467, time = 39s\n",
      "Epoch 52/120, train_cost=46.529, train_per=0.424, test_cost=52.043, test_per=0.463, time = 39s\n",
      "Epoch 53/120, train_cost=45.587, train_per=0.425, test_cost=51.399, test_per=0.452, time = 39s\n",
      "Epoch 54/120, train_cost=44.534, train_per=0.422, test_cost=50.926, test_per=0.452, time = 39s\n",
      "Epoch 55/120, train_cost=42.524, train_per=0.394, test_cost=50.525, test_per=0.436, time = 39s\n",
      "Epoch 56/120, train_cost=43.849, train_per=0.396, test_cost=50.043, test_per=0.429, time = 39s\n",
      "Epoch 57/120, train_cost=44.253, train_per=0.369, test_cost=49.547, test_per=0.413, time = 39s\n",
      "Epoch 58/120, train_cost=42.493, train_per=0.378, test_cost=49.289, test_per=0.411, time = 39s\n",
      "Epoch 59/120, train_cost=39.986, train_per=0.366, test_cost=48.826, test_per=0.413, time = 39s\n",
      "Epoch 60/120, train_cost=41.617, train_per=0.352, test_cost=48.454, test_per=0.402, time = 39s\n",
      "Epoch 61/120, train_cost=41.364, train_per=0.356, test_cost=48.218, test_per=0.404, time = 39s\n",
      "Epoch 62/120, train_cost=40.237, train_per=0.348, test_cost=47.788, test_per=0.395, time = 39s\n",
      "Epoch 63/120, train_cost=38.079, train_per=0.337, test_cost=47.485, test_per=0.398, time = 39s\n",
      "Epoch 64/120, train_cost=39.088, train_per=0.336, test_cost=47.441, test_per=0.391, time = 39s\n",
      "Epoch 65/120, train_cost=40.134, train_per=0.338, test_cost=47.126, test_per=0.386, time = 39s\n",
      "Epoch 66/120, train_cost=39.249, train_per=0.329, test_cost=46.646, test_per=0.386, time = 39s\n",
      "Epoch 67/120, train_cost=38.300, train_per=0.334, test_cost=46.728, test_per=0.387, time = 39s\n",
      "Epoch 68/120, train_cost=37.792, train_per=0.328, test_cost=46.238, test_per=0.385, time = 39s\n",
      "Epoch 69/120, train_cost=39.093, train_per=0.333, test_cost=46.180, test_per=0.379, time = 39s\n",
      "Epoch 70/120, train_cost=37.574, train_per=0.312, test_cost=45.802, test_per=0.368, time = 39s\n",
      "Epoch 71/120, train_cost=35.767, train_per=0.311, test_cost=45.763, test_per=0.369, time = 39s\n",
      "Epoch 72/120, train_cost=36.297, train_per=0.299, test_cost=45.601, test_per=0.362, time = 39s\n",
      "Epoch 73/120, train_cost=33.664, train_per=0.297, test_cost=45.412, test_per=0.363, time = 39s\n",
      "Epoch 74/120, train_cost=34.558, train_per=0.291, test_cost=45.186, test_per=0.363, time = 39s\n",
      "Epoch 75/120, train_cost=35.380, train_per=0.289, test_cost=45.301, test_per=0.362, time = 39s\n",
      "Epoch 76/120, train_cost=32.305, train_per=0.283, test_cost=45.207, test_per=0.362, time = 39s\n",
      "Epoch 77/120, train_cost=33.901, train_per=0.280, test_cost=45.070, test_per=0.354, time = 39s\n",
      "Epoch 78/120, train_cost=34.252, train_per=0.281, test_cost=44.953, test_per=0.356, time = 40s\n",
      "Epoch 79/120, train_cost=33.266, train_per=0.287, test_cost=44.680, test_per=0.360, time = 40s\n",
      "Epoch 80/120, train_cost=31.617, train_per=0.282, test_cost=44.828, test_per=0.348, time = 39s\n",
      "Epoch 81/120, train_cost=30.583, train_per=0.259, test_cost=44.769, test_per=0.352, time = 39s\n",
      "Epoch 82/120, train_cost=31.162, train_per=0.273, test_cost=44.722, test_per=0.357, time = 40s\n",
      "Epoch 83/120, train_cost=30.432, train_per=0.260, test_cost=44.745, test_per=0.350, time = 39s\n",
      "Epoch 84/120, train_cost=32.642, train_per=0.273, test_cost=44.643, test_per=0.348, time = 39s\n",
      "Epoch 85/120, train_cost=30.595, train_per=0.263, test_cost=44.623, test_per=0.348, time = 39s\n",
      "Epoch 86/120, train_cost=30.391, train_per=0.264, test_cost=44.575, test_per=0.351, time = 39s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/120, train_cost=30.387, train_per=0.254, test_cost=44.724, test_per=0.346, time = 39s\n",
      "Epoch 88/120, train_cost=28.168, train_per=0.236, test_cost=44.612, test_per=0.346, time = 39s\n",
      "Epoch 89/120, train_cost=29.788, train_per=0.251, test_cost=44.565, test_per=0.341, time = 39s\n",
      "Epoch 90/120, train_cost=29.034, train_per=0.253, test_cost=44.632, test_per=0.345, time = 40s\n",
      "Epoch 91/120, train_cost=27.195, train_per=0.230, test_cost=44.562, test_per=0.343, time = 39s\n",
      "Epoch 92/120, train_cost=26.976, train_per=0.240, test_cost=44.759, test_per=0.341, time = 40s\n",
      "Epoch 93/120, train_cost=28.978, train_per=0.239, test_cost=44.555, test_per=0.343, time = 40s\n",
      "Epoch 94/120, train_cost=27.863, train_per=0.240, test_cost=44.757, test_per=0.345, time = 39s\n",
      "Epoch 95/120, train_cost=27.588, train_per=0.228, test_cost=44.866, test_per=0.340, time = 39s\n",
      "Epoch 96/120, train_cost=27.759, train_per=0.242, test_cost=44.817, test_per=0.345, time = 39s\n",
      "Epoch 97/120, train_cost=28.611, train_per=0.237, test_cost=44.786, test_per=0.341, time = 39s\n",
      "Epoch 98/120, train_cost=26.401, train_per=0.225, test_cost=44.887, test_per=0.337, time = 39s\n",
      "Epoch 99/120, train_cost=26.684, train_per=0.226, test_cost=45.084, test_per=0.339, time = 40s\n",
      "Epoch 100/120, train_cost=25.625, train_per=0.231, test_cost=45.065, test_per=0.336, time = 40s\n",
      "Epoch 101/120, train_cost=24.682, train_per=0.220, test_cost=45.171, test_per=0.338, time = 40s\n",
      "Epoch 102/120, train_cost=27.221, train_per=0.228, test_cost=45.332, test_per=0.334, time = 41s\n",
      "Epoch 103/120, train_cost=24.455, train_per=0.211, test_cost=45.225, test_per=0.339, time = 40s\n",
      "Epoch 104/120, train_cost=25.177, train_per=0.217, test_cost=45.218, test_per=0.338, time = 39s\n",
      "Epoch 105/120, train_cost=25.272, train_per=0.208, test_cost=45.622, test_per=0.337, time = 39s\n",
      "Epoch 106/120, train_cost=25.440, train_per=0.202, test_cost=45.573, test_per=0.336, time = 39s\n",
      "Epoch 107/120, train_cost=24.940, train_per=0.201, test_cost=45.874, test_per=0.334, time = 40s\n",
      "Epoch 108/120, train_cost=24.235, train_per=0.204, test_cost=45.695, test_per=0.337, time = 42s\n",
      "Epoch 109/120, train_cost=23.721, train_per=0.202, test_cost=46.179, test_per=0.336, time = 42s\n",
      "Epoch 110/120, train_cost=24.257, train_per=0.201, test_cost=45.936, test_per=0.336, time = 40s\n",
      "Epoch 111/120, train_cost=23.674, train_per=0.201, test_cost=46.079, test_per=0.337, time = 41s\n",
      "Epoch 112/120, train_cost=24.221, train_per=0.202, test_cost=46.084, test_per=0.335, time = 40s\n",
      "Epoch 113/120, train_cost=23.329, train_per=0.188, test_cost=46.520, test_per=0.336, time = 40s\n",
      "Epoch 114/120, train_cost=23.226, train_per=0.190, test_cost=46.286, test_per=0.335, time = 40s\n",
      "Epoch 115/120, train_cost=23.084, train_per=0.193, test_cost=46.429, test_per=0.335, time = 40s\n",
      "Epoch 116/120, train_cost=22.729, train_per=0.190, test_cost=46.787, test_per=0.336, time = 39s\n",
      "Epoch 117/120, train_cost=23.173, train_per=0.199, test_cost=46.776, test_per=0.339, time = 41s\n",
      "Epoch 118/120, train_cost=22.203, train_per=0.192, test_cost=46.860, test_per=0.338, time = 40s\n",
      "Epoch 119/120, train_cost=22.492, train_per=0.195, test_cost=47.110, test_per=0.338, time = 40s\n",
      "Epoch 120/120, train_cost=22.003, train_per=0.184, test_cost=47.102, test_per=0.336, time = 40s\n"
     ]
    }
   ],
   "source": [
    "epochs = 120\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        batchRandIdx = np.random.permutation(len(train_batchedData))\n",
    "        \n",
    "        train_cost = train_per = 0\n",
    "        for batch, batchOrgI in enumerate(batchRandIdx):\n",
    "            batchInputs, batchTargetSparse, batchSeqLengths = train_batchedData[batchOrgI]\n",
    "            batchTargetIdx, batchTargetVals, batchTargetShape = batchTargetSparse\n",
    "            feedDict = {inputs: batchInputs, targetIdx: batchTargetIdx, targetVals: batchTargetVals,\n",
    "                        targetShape: batchTargetShape, seq_len: batchSeqLengths}\n",
    "            optimizer.run(feed_dict=feedDict)\n",
    "            \n",
    "            if batch == len(batchRandIdx) - 1:\n",
    "                train_cost, train_per = sess.run([cost, per], feedDict)\n",
    "            \n",
    "        test_cost = test_per = 0\n",
    "        for i in range(len(test_batchedData)):\n",
    "            batchInputs, batchTargetSparse, batchSeqLengths = test_batchedData[i]\n",
    "            batchTargetIdx, batchTargetVals, batchTargetShape = batchTargetSparse\n",
    "            feedDict = {inputs: batchInputs, targetIdx: batchTargetIdx, targetVals: batchTargetVals,\n",
    "                        targetShape: batchTargetShape, seq_len: batchSeqLengths}\n",
    "            \n",
    "            batch_cost, batch_per = sess.run([cost, per], feedDict)\n",
    "            test_cost += batch_cost\n",
    "            test_per += batch_per\n",
    "        \n",
    "        test_cost /= len(test_batchedData)\n",
    "        test_per /= len(test_batchedData)\n",
    "        \n",
    "        end = time.time()\n",
    "        log = \"Epoch {}/{}, train_cost={:.3f}, train_per={:.3f}, test_cost={:.3f}, test_per={:.3f}, time = {:.0f}s\"\n",
    "        print(log.format(epoch+1, epochs, train_cost, train_per, test_cost, test_per, end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
