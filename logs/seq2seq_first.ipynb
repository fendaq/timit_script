{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo\n",
    "- data shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phn_61 = ['aa', 'ae', 'ah', 'ao', 'aw', 'ax', 'ax-h', 'axr', 'ay', 'b', 'bcl', 'ch', 'd', 'dcl', 'dh', 'dx', 'eh', 'el', 'em', 'en', 'eng', 'epi', 'er', 'ey', 'f', 'g', 'gcl', 'h#', 'hh', 'hv', 'ih', 'ix', 'iy', 'jh', 'k', 'kcl', 'l', 'm', 'n', 'ng', 'nx', 'ow', 'oy', 'p', 'pau', 'pcl', 'q', 'r', 's', 'sh', 't', 'tcl', 'th', 'uh', 'uw', 'ux', 'v', 'w', 'y', 'z', 'zh']\n",
    "phn_39 = ['ae', 'ao', 'aw', 'ax', 'ay', 'b', 'ch', 'd', 'dh', 'dx', 'eh', 'er', 'ey', 'f', 'g', 'h#', 'hh', 'ix', 'iy', 'jh', 'k', 'l', 'm', 'n', 'ng', 'ow', 'oy', 'p', 'r', 's', 't', 'th', 'uh', 'uw', 'v', 'w', 'y', 'z', 'zh']\n",
    "mapping = {'ah': 'ax', 'ax-h': 'ax', 'ux': 'uw', 'aa': 'ao', 'ih': 'ix', 'axr': 'er', 'el': 'l', 'em': 'm', 'en': 'n', 'nx': 'n', 'eng': 'ng', 'sh': 'zh', 'hv': 'hh', 'bcl': 'h#', 'pcl': 'h#', 'dcl': 'h#', 'tcl': 'h#', 'gcl': 'h#', 'kcl': 'h#', 'q': 'h#', 'epi': 'h#', 'pau': 'h#'}\n",
    "\n",
    "train_featPath = os.path.join('data','train','feats','mfcc')\n",
    "train_labelPath = os.path.join('data','train','labels')\n",
    "test_featPath = os.path.join('data','test','feats','mfcc')\n",
    "test_labelPath = os.path.join('data','test','labels')\n",
    "\n",
    "feat_type = 'mfcc'\n",
    "feats_dim = 39 if feat_type=='mfcc' else 123 # filter bank\n",
    "num_classes = len(phn_61)\n",
    "sos_token_idx = num_classes\n",
    "eos_token_idx = num_classes + 1\n",
    "voca_size = num_classes + 2\n",
    "\n",
    "num_unit_encoder = 128\n",
    "num_unit_decoder = 128\n",
    "learning_rate = 0.001\n",
    "n_hidden_layer = 3\n",
    "decoder_embedding_dim = 16\n",
    "beam_width = 5\n",
    "batch_size = 128\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_batched_data(featPath, labelPath, batchSize):\n",
    "    ''' return 3-element tuple: batched data (list), maxTimeLengths (list), total number of samples (int)\n",
    "        The shape of batched_data's elements is (batchSize, maxLength, nFeatures)\n",
    "    '''\n",
    "    def data_lists_to_batches(inputList, targetList, batchSize):\n",
    "        nFeatures = inputList[0].shape[1]\n",
    "        input_target = list(zip(inputList, targetList))\n",
    "        input_target.sort(key=lambda x: x[0].shape[0])\n",
    "        inputList = []\n",
    "        targetList = []\n",
    "        for inp, tar in input_target:\n",
    "            inputList.append(inp)\n",
    "            targetList.append(tar)\n",
    "            \n",
    "        targetInList = []\n",
    "        targetOutList = []\n",
    "        for tar in targetList:\n",
    "            targetInList.append(np.pad(tar, (1,0), 'constant', constant_values=sos_token_idx))\n",
    "            targetOutList.append(np.pad(tar, (0,1), 'constant', constant_values=eos_token_idx))\n",
    "    \n",
    "        start, end = (0, batchSize)\n",
    "        dataBatches = []\n",
    "        maxLengths = []\n",
    "\n",
    "        while end <= len(inputList):\n",
    "            sourceSeqLengths = np.zeros(batchSize)\n",
    "            targetSeqLengths = np.zeros(batchSize)\n",
    "                \n",
    "            maxLength = [0, 0]\n",
    "            for batchI, i in enumerate(range(start, end)):\n",
    "                sourceSeqLengths[batchI] = inputList[i].shape[0]\n",
    "                maxLength[0] = max(maxLength[0], inputList[i].shape[0])\n",
    "            for batchI, i in enumerate(range(start, end)):\n",
    "                targetSeqLengths[batchI] = len(targetInList[i])\n",
    "                maxLength[1] = max(maxLength[1], len(targetInList[i]))\n",
    "            maxLengths.append(maxLength)\n",
    "\n",
    "            batchInputs = np.zeros((batchSize, maxLength[0], nFeatures))\n",
    "            batchTargetIn = np.zeros((batchSize, maxLength[1]))\n",
    "            batchTargetOut = np.zeros((batchSize, maxLength[1]))\n",
    "            for batchI, i in enumerate(range(start, end)):\n",
    "                padSecs = maxLength[0] - inputList[i].shape[0]\n",
    "                batchInputs[batchI,:,:] = np.pad(inputList[i], ((0,padSecs),(0,0)), 'constant', constant_values=0)\n",
    "                \n",
    "                padSecs = maxLength[1] - len(targetInList[i])\n",
    "                batchTargetIn[batchI, :] = np.pad(targetInList[i], (0, padSecs), 'constant', constant_values=eos_token_idx)\n",
    "                batchTargetOut[batchI, :] = np.pad(targetOutList[i], (0, padSecs), 'constant', constant_values=eos_token_idx)\n",
    "            dataBatches.append((batchInputs, batchTargetIn, batchTargetOut, sourceSeqLengths, targetSeqLengths))\n",
    "            start += batchSize\n",
    "            end += batchSize\n",
    "            \n",
    "            if end > len(inputList) and start < len(inputList):\n",
    "                end = len(inputList)\n",
    "                batchSize = end - start\n",
    "            \n",
    "        return (dataBatches, maxLengths)\n",
    "    \n",
    "    return data_lists_to_batches([np.load(os.path.join(featPath, fn)) for fn in os.listdir(featPath)],\n",
    "                                [np.load(os.path.join(labelPath, fn)) for fn in os.listdir(labelPath)],\n",
    "                                batchSize) + (len(os.listdir(featPath)),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batchedData, train_maxTimeLengths, train_totalN = load_batched_data(train_featPath, train_labelPath, batch_size)\n",
    "test_batchedData, test_maxTimeLengths, test_totalN = load_batched_data(test_featPath, test_labelPath, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phn_61_tensor = tf.constant(phn_61, dtype=tf.string)\n",
    "phn_39_tensor = tf.constant(phn_39, dtype=tf.string)\n",
    "mapping_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(list(mapping.keys()), list(mapping.values())), default_value='')\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, shape=[None, None, feats_dim])\n",
    "targets_in = tf.placeholder(tf.int32, shape=[None, None])\n",
    "targets_out = tf.placeholder(tf.int32, shape=[None, None])\n",
    "source_seq_len = tf.placeholder(tf.int32, shape=[None])\n",
    "target_seq_len = tf.placeholder(tf.int32, shape=[None])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.variable_scope('root'):\n",
    "    batch_size_tensor = tf.shape(inputs)[0]\n",
    "    embedding_decoder = tf.Variable(tf.random_uniform([voca_size, decoder_embedding_dim], dtype=tf.float32))\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(embedding_decoder, targets_in)\n",
    "\n",
    "    cells_fw = [tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.GRUCell(num_unit_encoder, kernel_initializer=tf.orthogonal_initializer()), output_keep_prob=keep_prob) for _ in range(n_hidden_layer)]\n",
    "    cells_bw = [tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.GRUCell(num_unit_encoder, kernel_initializer=tf.orthogonal_initializer()), output_keep_prob=keep_prob) for _ in range(n_hidden_layer)]\n",
    "    encoder_outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(cells_fw, cells_bw, inputs, sequence_length=source_seq_len, dtype=tf.float32)\n",
    "\n",
    "    memory = encoder_outputs\n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(num_unit_decoder, memory, memory_sequence_length=source_seq_len)\n",
    "    #decoder_cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.GRUCell(num_unit_decoder), output_keep_prob=keep_prob) for _ in range(n_hidden_layer)])\n",
    "    decoder_cell = tf.nn.rnn_cell.GRUCell(num_unit_decoder, kernel_initializer=tf.orthogonal_initializer())\n",
    "    att_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism, attention_layer_size=num_unit_decoder)\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inp, target_seq_len, time_major=False)\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(att_decoder_cell, helper, initial_state=att_decoder_cell.zero_state(batch_size_tensor, dtype=tf.float32))\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=False)\n",
    "\n",
    "    output_layer = tf.contrib.keras.layers.Dense(voca_size, use_bias=False)\n",
    "    logits = output_layer(outputs.rnn_output)\n",
    "    max_time = tf.shape(targets_out)[1]\n",
    "    target_weights = tf.sequence_mask(target_seq_len, max_time, dtype=logits.dtype)\n",
    "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets_out, logits=logits)\n",
    "    loss = tf.reduce_sum(crossent * target_weights) / tf.to_float(batch_size_tensor)\n",
    "\n",
    "    params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(loss, params)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    update_step = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "\n",
    "with tf.variable_scope('root', reuse=True):\n",
    "    tiled_memory = tf.contrib.seq2seq.tile_batch(memory, multiplier=beam_width)\n",
    "    tiled_source_seq_len = tf.contrib.seq2seq.tile_batch(source_seq_len, multiplier=beam_width)\n",
    "    tiled_batch_size = batch_size_tensor * beam_width\n",
    "    tiled_attention_mechanism = tf.contrib.seq2seq.LuongAttention(num_unit_decoder, tiled_memory, memory_sequence_length=tiled_source_seq_len)\n",
    "    tiled_att_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, tiled_attention_mechanism, attention_layer_size=num_unit_decoder)\n",
    "    decoder_initial_state = tiled_att_decoder_cell.zero_state(tiled_batch_size, tf.float32)\n",
    "\n",
    "    beam_decoder = tf.contrib.seq2seq.BeamSearchDecoder(tiled_att_decoder_cell, embedding_decoder, tf.fill([batch_size_tensor], sos_token_idx),\n",
    "                                                       eos_token_idx, decoder_initial_state, beam_width, output_layer=output_layer)\n",
    "    decoded, _, final_seq_len = tf.contrib.seq2seq.dynamic_decode(beam_decoder, maximum_iterations=100, output_time_major=False)\n",
    "    predicted_ids = decoded.predicted_ids # shape = [batch, max_time, beam_width]\n",
    "    \n",
    "    def map_to_reduced_phn(p):\n",
    "        val = mapping_table.lookup(phn_61_tensor[p])\n",
    "        f1 = lambda: tf.to_int32(tf.reduce_min(tf.where(tf.equal(val, phn_39_tensor))))\n",
    "        f2 = lambda: tf.to_int32(tf.reduce_min(tf.where(tf.equal(phn_61_tensor[p], phn_39_tensor))))\n",
    "        return tf.cond(tf.not_equal(val, ''), f1, f2)\n",
    "    \n",
    "    indices = tf.to_int64(tf.where(tf.logical_and(tf.not_equal(predicted_ids[:,:,0], -1), tf.not_equal(predicted_ids[:,:,0], eos_token_idx))))\n",
    "    vals = tf.to_int32(tf.gather_nd(predicted_ids[:,:,0], indices))\n",
    "    shape = tf.to_int64(tf.shape(predicted_ids[:,:,0]))\n",
    "    decoded_sparse = tf.SparseTensor(indices, tf.map_fn(map_to_reduced_phn, vals), shape)\n",
    "    \n",
    "    indices = tf.to_int64(tf.where(tf.not_equal(targets_out, eos_token_idx)))\n",
    "    vals = tf.to_int32(tf.gather_nd(targets_out, indices))\n",
    "    shape = tf.to_int64(tf.shape(targets_out))\n",
    "    targets_out_sparse = tf.SparseTensor(indices, tf.map_fn(map_to_reduced_phn, vals), shape)\n",
    "\n",
    "    per = tf.reduce_sum(tf.edit_distance(decoded_sparse, targets_out_sparse, normalize=False)) / tf.to_float(tf.size(targets_out_sparse.values))\n",
    "    \n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, train_cost=109.563, test_cost=151.554, time = 59s\n",
      "Epoch 2/100, train_cost=81.121, test_cost=144.629, time = 58s\n",
      "Epoch 3/100, train_cost=164.809, test_cost=133.549, time = 58s\n",
      "Epoch 4/100, train_cost=153.571, test_cost=124.007, time = 58s\n",
      "Epoch 5/100, train_cost=94.404, test_cost=120.609, time = 58s\n",
      "Epoch 6/100, train_cost=175.919, test_cost=117.658, time = 59s\n",
      "Epoch 7/100, train_cost=91.327, test_cost=113.345, time = 58s\n",
      "Epoch 8/100, train_cost=92.332, test_cost=110.889, time = 59s\n",
      "Epoch 9/100, train_cost=132.245, test_cost=106.820, time = 58s\n",
      "Epoch 10/100, train_cost=84.041, test_cost=104.936, time = 58s\n",
      "Epoch 11/100, train_cost=91.236, test_cost=96.582, time = 59s\n",
      "Epoch 12/100, train_cost=110.556, test_cost=89.209, time = 59s\n",
      "Epoch 13/100, train_cost=65.421, test_cost=85.337, time = 59s\n",
      "Epoch 14/100, train_cost=59.316, test_cost=79.227, time = 59s\n",
      "Epoch 15/100, train_cost=53.269, test_cost=71.181, time = 58s\n",
      "Epoch 16/100, train_cost=91.395, test_cost=65.152, time = 59s\n",
      "Epoch 17/100, train_cost=69.355, test_cost=62.867, time = 59s\n",
      "Epoch 18/100, train_cost=22.198, test_cost=58.349, time = 59s\n",
      "Epoch 19/100, train_cost=81.686, test_cost=56.111, time = 59s\n",
      "Epoch 20/100, train_cost=41.728, test_cost=54.102, time = 59s\n",
      "Epoch 21/100, train_cost=44.243, test_cost=52.286, time = 59s\n",
      "Epoch 22/100, train_cost=31.524, test_cost=49.217, time = 58s\n",
      "Epoch 23/100, train_cost=46.022, test_cost=46.968, time = 59s\n",
      "Epoch 24/100, train_cost=35.930, test_cost=47.196, time = 58s\n",
      "Epoch 25/100, train_cost=26.750, test_cost=44.288, time = 58s\n",
      "Epoch 26/100, train_cost=25.050, test_cost=44.020, time = 58s\n",
      "Epoch 27/100, train_cost=33.357, test_cost=43.043, time = 58s\n",
      "Epoch 28/100, train_cost=23.425, test_cost=41.287, time = 58s\n",
      "Epoch 29/100, train_cost=23.054, test_cost=40.830, time = 58s\n",
      "Epoch 30/100, train_cost=24.806, test_cost=40.963, time = 58s\n",
      "Epoch 31/100, train_cost=21.256, test_cost=39.217, time = 58s\n",
      "Epoch 32/100, train_cost=42.655, test_cost=39.152, time = 59s\n",
      "Epoch 33/100, train_cost=41.929, test_cost=38.617, time = 59s\n",
      "Epoch 34/100, train_cost=10.078, test_cost=37.833, time = 58s\n",
      "Epoch 35/100, train_cost=19.775, test_cost=37.849, time = 58s\n",
      "Epoch 36/100, train_cost=28.054, test_cost=37.712, time = 59s\n",
      "Epoch 37/100, train_cost=23.368, test_cost=36.464, time = 59s\n",
      "Epoch 38/100, train_cost=24.620, test_cost=36.739, time = 59s\n",
      "Epoch 39/100, train_cost=14.775, test_cost=36.462, time = 59s\n",
      "Epoch 40/100, train_cost=18.308, test_cost=36.294, time = 58s\n",
      "Epoch 41/100, train_cost=40.853, test_cost=35.743, time = 59s\n",
      "Epoch 42/100, train_cost=7.586, test_cost=37.038, time = 58s\n",
      "Epoch 43/100, train_cost=21.505, test_cost=36.456, time = 59s\n",
      "Epoch 44/100, train_cost=30.113, test_cost=35.877, time = 59s\n",
      "Epoch 45/100, train_cost=7.443, test_cost=36.086, time = 59s\n",
      "Epoch 46/100, train_cost=12.655, test_cost=35.828, time = 59s\n",
      "Epoch 47/100, train_cost=37.452, test_cost=36.251, time = 59s\n",
      "Epoch 48/100, train_cost=26.647, test_cost=35.922, time = 59s\n",
      "Epoch 49/100, train_cost=25.683, test_cost=35.684, time = 59s\n",
      "Epoch 50/100, train_cost=15.274, test_cost=37.537, time = 59s\n",
      "Epoch 51/100, train_cost=29.744, test_cost=36.574, time = 59s\n",
      "Epoch 52/100, train_cost=12.742, test_cost=36.739, time = 59s\n",
      "Epoch 53/100, train_cost=14.271, test_cost=36.350, time = 59s\n",
      "Epoch 54/100, train_cost=12.050, test_cost=36.332, time = 59s\n",
      "Epoch 55/100, train_cost=8.951, test_cost=37.637, time = 59s\n",
      "Epoch 56/100, train_cost=22.622, test_cost=36.638, time = 59s\n",
      "Epoch 57/100, train_cost=7.118, test_cost=37.548, time = 58s\n",
      "Epoch 58/100, train_cost=4.462, test_cost=37.685, time = 59s\n",
      "Epoch 59/100, train_cost=11.704, test_cost=37.740, time = 59s\n",
      "Epoch 60/100, train_cost=12.953, test_cost=37.788, time = 59s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-be5c9622deb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m             feedDict = {inputs: batchInputs, targets_in: batchTargetIn, targets_out: batchTargetOut,\n\u001b[0;32m     12\u001b[0m                         source_seq_len: sourceSeqLen, target_seq_len: targetSeqLen, keep_prob: 0.6}\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mupdate_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeedDict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchRandIdx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m   1698\u001b[0m         \u001b[0mnone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1699\u001b[0m     \"\"\"\n\u001b[1;32m-> 1700\u001b[1;33m     \u001b[0m_run_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[1;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   4071\u001b[0m                        \u001b[1;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4072\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 4073\u001b[1;33m   \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(mapping_table.init)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        batchRandIdx = np.random.permutation(len(train_batchedData))\n",
    "        \n",
    "        for batch, batchOrgI in enumerate(batchRandIdx):\n",
    "            batchInputs, batchTargetIn, batchTargetOut, sourceSeqLen, targetSeqLen = train_batchedData[batchOrgI]\n",
    "            feedDict = {inputs: batchInputs, targets_in: batchTargetIn, targets_out: batchTargetOut,\n",
    "                        source_seq_len: sourceSeqLen, target_seq_len: targetSeqLen, keep_prob: 0.6}\n",
    "            update_step.run(feed_dict=feedDict)\n",
    "            \n",
    "            if batch == len(batchRandIdx) - 1:\n",
    "                feedDict[keep_prob] = 1.0\n",
    "                train_cost = sess.run(loss, feedDict)\n",
    "            \n",
    "        test_cost = 0\n",
    "        for i in range(len(test_batchedData)):\n",
    "            batchInputs, batchTargetIn, batchTargetOut, sourceSeqLen, targetSeqLen = test_batchedData[i]\n",
    "            feedDict = {inputs: batchInputs, targets_in: batchTargetIn, targets_out: batchTargetOut,\n",
    "                        source_seq_len: sourceSeqLen, target_seq_len: targetSeqLen, keep_prob: 1.0}\n",
    "            \n",
    "            batch_cost = sess.run(loss, feedDict)\n",
    "            test_cost += batch_cost\n",
    "        test_cost /= len(test_batchedData)\n",
    "        \n",
    "        end = time.time()\n",
    "        log = \"Epoch {}/{}, train_cost={:.3f}, test_cost={:.3f}, time = {:.0f}s\"\n",
    "        print(log.format(epoch+1, epochs, train_cost, test_cost, end-start))\n",
    "        \n",
    "        if (epoch+1) % 5 == 0:\n",
    "            save_path = saver.save(sess, 'model/seq2seq/model.ckpt')\n",
    "            #print('Epoch {} - model saved in file: {}'.format(epoch+1, save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/seq2seq/model.ckpt\n",
      "Epoch 61/100, train_cost=11.305, train_per=0.069, test_cost=37.788, test_per=0.244, time = 229s\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, 'model/seq2seq/model.ckpt')\n",
    "    sess.run(mapping_table.init)\n",
    "    \n",
    "    start = time.time()\n",
    "    train_cost = train_per = 0\n",
    "    for i in range(len(train_batchedData)):\n",
    "            batchInputs, batchTargetIn, batchTargetOut, sourceSeqLen, targetSeqLen = train_batchedData[i]\n",
    "            feedDict = {inputs: batchInputs, targets_in: batchTargetIn, targets_out: batchTargetOut,\n",
    "                        source_seq_len: sourceSeqLen, target_seq_len: targetSeqLen, keep_prob: 1.0}\n",
    "            batch_cost, batch_per = sess.run([loss, per], feedDict)\n",
    "            train_cost += batch_cost\n",
    "            train_per += batch_per\n",
    "    \n",
    "    test_cost = test_per = 0\n",
    "    for i in range(len(test_batchedData)):\n",
    "            batchInputs, batchTargetIn, batchTargetOut, sourceSeqLen, targetSeqLen = test_batchedData[i]\n",
    "            feedDict = {inputs: batchInputs, targets_in: batchTargetIn, targets_out: batchTargetOut,\n",
    "                        source_seq_len: sourceSeqLen, target_seq_len: targetSeqLen, keep_prob: 1.0}\n",
    "            batch_cost, batch_per = sess.run([loss, per], feedDict)\n",
    "            test_cost += batch_cost\n",
    "            test_per += batch_per\n",
    "            \n",
    "    train_cost /= len(train_batchedData)\n",
    "    train_per /= len(train_batchedData)\n",
    "    test_cost /= len(test_batchedData)\n",
    "    test_per /= len(test_batchedData)\n",
    "    \n",
    "    end = time.time()\n",
    "    log = \"Epoch {}/{}, train_cost={:.3f}, train_per={:.3f}, test_cost={:.3f}, test_per={:.3f}, time = {:.0f}s\"\n",
    "    print(log.format(epoch+1, epochs, train_cost, train_per, test_cost, test_per, end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
