{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mfccPath = os.path.join('data','train','feats')\n",
    "train_labelPath = os.path.join('data','train','labels')\n",
    "test_mfccPath = os.path.join('data','test','feats')\n",
    "test_labelPath = os.path.join('data','test','labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_batched_data(mfccPath, labelPath, batchSize):\n",
    "    ''' return 3-element tuple: batched data (list), maxTimeLength (int), total number of samples (int)\n",
    "        The shape of batched_data's elements is (batchSize, maxLength, nFeatures)\n",
    "    '''\n",
    "    \n",
    "    def list_to_sparse_tensor(targetList):\n",
    "        indices = []\n",
    "        vals = []\n",
    "\n",
    "        for tI, target in enumerate(targetList):\n",
    "            for seqI, val in enumerate(target):\n",
    "                indices.append([tI, seqI])\n",
    "                vals.append(val)\n",
    "        shape = [len(targetList), np.asarray(indices).max(axis=0)[1]+1]\n",
    "        return (np.array(indices), np.array(vals), np.array(shape))\n",
    "    \n",
    "    def data_lists_to_batches(inputList, targetList, batchSize):\n",
    "        nFeatures = inputList[0].shape[1]\n",
    "        maxLength = 0\n",
    "        for inp in inputList:\n",
    "            maxLength = max(maxLength, inp.shape[0])\n",
    "\n",
    "        randIdx = np.random.permutation(len(inputList))\n",
    "        start, end = (0, batchSize)\n",
    "        dataBatches = []\n",
    "\n",
    "        while end <= len(inputList):\n",
    "            batchSeqLengths = np.zeros(batchSize)\n",
    "                \n",
    "            for batchI, origI in enumerate(randIdx[start:end]):\n",
    "                batchSeqLengths[batchI] = inputList[origI].shape[0]\n",
    "\n",
    "            batchInputs = np.zeros((batchSize, maxLength, nFeatures))\n",
    "            batchTargetList = []\n",
    "            for batchI, origI in enumerate(randIdx[start:end]):\n",
    "                padSecs = maxLength - inputList[origI].shape[0]\n",
    "                batchInputs[batchI,:,:] = np.pad(inputList[origI], ((0,padSecs),(0,0)), 'constant', constant_values=0)\n",
    "                batchTargetList.append(targetList[origI])\n",
    "            dataBatches.append((batchInputs, list_to_sparse_tensor(batchTargetList), batchSeqLengths))\n",
    "            start += batchSize\n",
    "            end += batchSize\n",
    "        return (dataBatches, maxLength)\n",
    "    \n",
    "    return data_lists_to_batches([np.load(os.path.join(mfccPath, fn)) for fn in os.listdir(mfccPath)],\n",
    "                                [np.load(os.path.join(labelPath, fn)) for fn in os.listdir(labelPath)],\n",
    "                                batchSize) + (len(os.listdir(mfccPath)),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchSize = 128\n",
    "num_features = 39\n",
    "num_hidden = 128\n",
    "num_classes = 39 + 1\n",
    "learning_rate = 0.001\n",
    "n_hidden_layer = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batchedData, train_maxTimeSteps, train_totalN = load_batched_data(train_mfccPath, train_labelPath, batchSize)\n",
    "test_batchedData, test_maxTimeSteps, test_totalN = load_batched_data(test_mfccPath, test_labelPath, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.float32, [None, None, num_features])\n",
    "targetIdx = tf.placeholder(tf.int64)\n",
    "targetVals = tf.placeholder(tf.int32)\n",
    "targetShape = tf.placeholder(tf.int64)\n",
    "targets = tf.SparseTensor(targetIdx, targetVals, targetShape)\n",
    "seq_len = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "cells_fw = [tf.nn.rnn_cell.LSTMCell(num_hidden, use_peepholes=True) for _ in range(n_hidden_layer)]\n",
    "cells_bw = [tf.nn.rnn_cell.LSTMCell(num_hidden, use_peepholes=True) for _ in range(n_hidden_layer)]\n",
    "outputs, _, _ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(cells_fw, cells_bw, inputs, dtype=tf.float32, sequence_length=seq_len)\n",
    "\n",
    "shape = tf.shape(outputs)\n",
    "outputs = tf.reshape(outputs, [-1, shape[2]])\n",
    "W = tf.Variable(tf.truncated_normal([num_hidden*2, num_classes], stddev=0.1))\n",
    "b = tf.Variable(tf.constant(0., shape=[num_classes]))\n",
    "logits = tf.matmul(outputs, W) + b\n",
    "logits = tf.reshape(logits, [shape[0], shape[1], num_classes])\n",
    "logits = tf.transpose(logits, [1,0,2])\n",
    "\n",
    "loss = tf.nn.ctc_loss(labels=targets, inputs=logits, sequence_length=seq_len, time_major=True)\n",
    "cost = tf.reduce_mean(loss)\n",
    "params = tf.trainable_variables()\n",
    "gradients = tf.gradients(cost, params)\n",
    "clipped_gradients, _ = tf.clip_by_global_norm(gradients, 1)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "update_step = optimizer.apply_gradients(zip(clipped_gradients, params))\n",
    "decoded, log_prob = tf.nn.ctc_greedy_decoder(logits, seq_len)\n",
    "per = tf.reduce_sum(tf.edit_distance(tf.to_int32(decoded[0]), targets, normalize=False)) / tf.to_float(tf.size(targets.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120, train_cost=122.026, train_per=1.000, test_cost=125.058, test_per=1.000, time = 145s\n",
      "Epoch 2/120, train_cost=111.099, train_per=0.974, test_cost=110.422, test_per=0.974, time = 147s\n",
      "Epoch 3/120, train_cost=80.893, train_per=0.899, test_cost=84.804, test_per=0.901, time = 148s\n",
      "Epoch 4/120, train_cost=69.064, train_per=0.801, test_cost=70.994, test_per=0.810, time = 149s\n",
      "Epoch 5/120, train_cost=63.202, train_per=0.762, test_cost=62.752, test_per=0.777, time = 148s\n",
      "Epoch 6/120, train_cost=54.602, train_per=0.620, test_cost=56.981, test_per=0.648, time = 149s\n",
      "Epoch 7/120, train_cost=48.360, train_per=0.533, test_cost=52.463, test_per=0.549, time = 148s\n",
      "Epoch 8/120, train_cost=46.615, train_per=0.475, test_cost=49.323, test_per=0.501, time = 149s\n",
      "Epoch 9/120, train_cost=41.187, train_per=0.411, test_cost=46.171, test_per=0.443, time = 149s\n",
      "Epoch 10/120, train_cost=37.116, train_per=0.376, test_cost=44.616, test_per=0.411, time = 149s\n",
      "Epoch 11/120, train_cost=33.094, train_per=0.321, test_cost=42.937, test_per=0.376, time = 149s\n",
      "Epoch 12/120, train_cost=33.481, train_per=0.297, test_cost=42.124, test_per=0.345, time = 148s\n",
      "Epoch 13/120, train_cost=31.698, train_per=0.275, test_cost=40.813, test_per=0.334, time = 148s\n",
      "Epoch 14/120, train_cost=27.231, train_per=0.251, test_cost=40.141, test_per=0.319, time = 149s\n",
      "Epoch 15/120, train_cost=26.834, train_per=0.242, test_cost=39.386, test_per=0.316, time = 149s\n",
      "Epoch 16/120, train_cost=27.106, train_per=0.229, test_cost=39.653, test_per=0.308, time = 148s\n",
      "Epoch 17/120, train_cost=22.538, train_per=0.197, test_cost=39.381, test_per=0.298, time = 149s\n",
      "Epoch 18/120, train_cost=23.998, train_per=0.204, test_cost=39.056, test_per=0.292, time = 152s\n",
      "Epoch 19/120, train_cost=22.883, train_per=0.199, test_cost=39.514, test_per=0.299, time = 156s\n",
      "Epoch 20/120, train_cost=21.111, train_per=0.180, test_cost=39.382, test_per=0.292, time = 157s\n",
      "Epoch 21/120, train_cost=19.056, train_per=0.171, test_cost=39.362, test_per=0.284, time = 155s\n",
      "Epoch 22/120, train_cost=18.469, train_per=0.158, test_cost=39.898, test_per=0.288, time = 156s\n",
      "Epoch 23/120, train_cost=15.916, train_per=0.134, test_cost=40.442, test_per=0.282, time = 153s\n",
      "Epoch 24/120, train_cost=16.243, train_per=0.142, test_cost=40.880, test_per=0.284, time = 153s\n",
      "Epoch 25/120, train_cost=14.646, train_per=0.129, test_cost=41.198, test_per=0.285, time = 151s\n",
      "Epoch 26/120, train_cost=13.998, train_per=0.110, test_cost=41.663, test_per=0.283, time = 156s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5e939fc736ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m             feedDict = {inputs: batchInputs, targetIdx: batchTargetIdx, targetVals: batchTargetVals,\n\u001b[0;32m     15\u001b[0m                         targetShape: batchTargetShape, seq_len: batchSeqLengths}\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mupdate_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeedDict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchRandIdx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m   1704\u001b[0m         \u001b[0mnone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1705\u001b[0m     \"\"\"\n\u001b[1;32m-> 1706\u001b[1;33m     \u001b[0m_run_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[1;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3961\u001b[0m                        \u001b[1;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3962\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 3963\u001b[1;33m   \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 120\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        batchRandIdx = np.random.permutation(len(train_batchedData))\n",
    "        \n",
    "        train_cost = train_per = 0\n",
    "        for batch, batchOrgI in enumerate(batchRandIdx):\n",
    "            batchInputs, batchTargetSparse, batchSeqLengths = train_batchedData[batchOrgI]\n",
    "            batchTargetIdx, batchTargetVals, batchTargetShape = batchTargetSparse\n",
    "            feedDict = {inputs: batchInputs, targetIdx: batchTargetIdx, targetVals: batchTargetVals,\n",
    "                        targetShape: batchTargetShape, seq_len: batchSeqLengths}\n",
    "            update_step.run(feed_dict=feedDict)\n",
    "            \n",
    "            if batch == len(batchRandIdx) - 1:\n",
    "                train_cost, train_per = sess.run([cost, per], feedDict)\n",
    "            \n",
    "        test_cost = test_per = 0\n",
    "        for i in range(len(test_batchedData)):\n",
    "            batchInputs, batchTargetSparse, batchSeqLengths = test_batchedData[i]\n",
    "            batchTargetIdx, batchTargetVals, batchTargetShape = batchTargetSparse\n",
    "            feedDict = {inputs: batchInputs, targetIdx: batchTargetIdx, targetVals: batchTargetVals,\n",
    "                        targetShape: batchTargetShape, seq_len: batchSeqLengths}\n",
    "            \n",
    "            batch_cost, batch_per = sess.run([cost, per], feedDict)\n",
    "            test_cost += batch_cost\n",
    "            test_per += batch_per\n",
    "        \n",
    "        test_cost /= len(test_batchedData)\n",
    "        test_per /= len(test_batchedData)\n",
    "        \n",
    "        end = time.time()\n",
    "        log = \"Epoch {}/{}, train_cost={:.3f}, train_per={:.3f}, test_cost={:.3f}, test_per={:.3f}, time = {:.0f}s\"\n",
    "        print(log.format(epoch+1, epochs, train_cost, train_per, test_cost, test_per, end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
