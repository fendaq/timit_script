{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- need aggressive dropout\n",
    "- prefer simple architecture, modest size node\n",
    "- fine tuning, should monitor per step error not epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phn_61 = ['aa', 'ae', 'ah', 'ao', 'aw', 'ax', 'ax-h', 'axr', 'ay', 'b', 'bcl', 'ch', 'd', 'dcl', 'dh', 'dx', 'eh', 'el', 'em', 'en', 'eng', 'epi', 'er', 'ey', 'f', 'g', 'gcl', 'h#', 'hh', 'hv', 'ih', 'ix', 'iy', 'jh', 'k', 'kcl', 'l', 'm', 'n', 'ng', 'nx', 'ow', 'oy', 'p', 'pau', 'pcl', 'q', 'r', 's', 'sh', 't', 'tcl', 'th', 'uh', 'uw', 'ux', 'v', 'w', 'y', 'z', 'zh']\n",
    "phn_39 = ['ae', 'ao', 'aw', 'ax', 'ay', 'b', 'ch', 'd', 'dh', 'dx', 'eh', 'er', 'ey', 'f', 'g', 'h#', 'hh', 'ix', 'iy', 'jh', 'k', 'l', 'm', 'n', 'ng', 'ow', 'oy', 'p', 'r', 's', 't', 'th', 'uh', 'uw', 'v', 'w', 'y', 'z', 'zh']\n",
    "mapping = {'ah': 'ax', 'ax-h': 'ax', 'ux': 'uw', 'aa': 'ao', 'ih': 'ix', 'axr': 'er', 'el': 'l', 'em': 'm', 'en': 'n', 'nx': 'n', 'eng': 'ng', 'sh': 'zh', 'hv': 'hh', 'bcl': 'h#', 'pcl': 'h#', 'dcl': 'h#', 'tcl': 'h#', 'gcl': 'h#', 'kcl': 'h#', 'q': 'h#', 'epi': 'h#', 'pau': 'h#'}\n",
    "\n",
    "TRAIN_FILE = './data/fbank/train.tfrecords'\n",
    "DEV_FILE = './data/fbank/dev.tfrecords'\n",
    "TEST_FILE = './data/fbank/test.tfrecords'\n",
    "checkpoints_path = './model/seq2seq_residual/ckpt'\n",
    "ft_checkpoints_path = './model/seq2seq_residual/finetunning/ckpt'\n",
    "final_checkpoints_path = './model/seq2seq_residual/final/ckpt'\n",
    "\n",
    "feat_type = 'fbank'\n",
    "feats_dim = 39 if feat_type=='mfcc' else 123\n",
    "labels_sos_id = len(phn_61)\n",
    "labels_eos_id = len(phn_61) + 1\n",
    "num_classes = len(phn_61) + 2\n",
    "\n",
    "num_unit_encoder = 128\n",
    "num_unit_decoder = 256\n",
    "learning_rate = 0.001\n",
    "n_hidden_layer = 3\n",
    "beam_width = 10\n",
    "batch_size = 32\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, batch_size, num_unit_encoder, num_unit_decoder, n_hidden_layer, feats_dim, num_classes, labels_sos_id, labels_eos_id,\n",
    "                 learning_rate=0.001, optimizer=None, beam_width=10, phn_61=None, phn_39=None, mapping=None, file_type=None, model_type=None):\n",
    "        iterator = self._get_iterator(batch_size, feats_dim, labels_sos_id, labels_eos_id, file_type, model_type)\n",
    "        self.iterator_initializer = iterator.initializer\n",
    "        \n",
    "        batched_data = iterator.get_next()\n",
    "        features = batched_data[0]\n",
    "        labels_in = batched_data[1]\n",
    "        labels_out = batched_data[2]\n",
    "        feats_seq_len = tf.to_int32(batched_data[3])\n",
    "        labels_in_seq_len = tf.to_int32(batched_data[4])\n",
    "        \n",
    "        decoded = self._compute_dynamic_decode(features, labels_in, feats_seq_len, labels_in_seq_len, num_unit_encoder,\n",
    "                                               num_unit_decoder, n_hidden_layer, num_classes, beam_width, model_type)\n",
    "        self.saver = tf.train.Saver() # save graph until decoded\n",
    "        \n",
    "        if model_type == 'train':\n",
    "            logits = decoded\n",
    "            self.loss = self._compute_loss(labels_out, labels_in_seq_len, logits)\n",
    "            self.update_step = self._get_update_step(self.loss, learning_rate, optimizer)\n",
    "        elif model_type == 'eval':\n",
    "            logits = decoded\n",
    "            self.loss = self._compute_loss(labels_out, labels_in_seq_len, logits)\n",
    "        else: # model_type == 'infer'\n",
    "            predicted_ids = decoded\n",
    "            self.per = self._compute_per(predicted_ids, labels_out, labels_eos_id, phn_61, phn_39, mapping)\n",
    "            \n",
    "    def _get_sparse_tensor(self, dense, default):\n",
    "        indices = tf.to_int64(tf.where(tf.not_equal(dense, default)))\n",
    "        vals = tf.to_int32(tf.gather_nd(dense, indices))\n",
    "        shape = tf.to_int64(tf.shape(dense))\n",
    "        return tf.SparseTensor(indices, vals, shape)\n",
    "        \n",
    "    def _get_iterator(self, batch_size, feats_dim, labels_sos_id, labels_eos_id, file_type, model_type):\n",
    "        dataset = tf.contrib.data.TFRecordDataset(file_type)\n",
    "        context_features = {'feats_seq_len': tf.FixedLenFeature([], dtype=tf.int64),\n",
    "                           'labels_seq_len': tf.FixedLenFeature([], dtype=tf.int64)}\n",
    "        sequence_features = {'features': tf.FixedLenSequenceFeature([feats_dim], dtype=tf.float32),\n",
    "                            'labels': tf.FixedLenSequenceFeature([], dtype=tf.int64)}\n",
    "        dataset = dataset.map(lambda serialized_example: tf.parse_single_sequence_example(serialized_example,\n",
    "                                                                                         context_features=context_features,\n",
    "                                                                                         sequence_features=sequence_features))\n",
    "        dataset = dataset.map(lambda context, sequence: (sequence['features'], sequence['labels'], \n",
    "                                                         context['feats_seq_len'], context['labels_seq_len']))\n",
    "        dataset = dataset.map(lambda features, labels, feats_seq_len, labels_seq_len: (features, \n",
    "                                                        tf.concat(([labels_sos_id], labels),0),\n",
    "                                                        tf.concat((labels, [labels_eos_id]), 0),\n",
    "                                                                feats_seq_len, labels_seq_len))\n",
    "        dataset = dataset.map(lambda features, labels_in, labels_out, feats_seq_len, labels_seq_len: \n",
    "                                          (features, labels_in, labels_out, feats_seq_len, tf.size(labels_in, out_type=tf.int64)))\n",
    "        def batching_func(x):\n",
    "            return x.padded_batch(batch_size,\n",
    "                                 padded_shapes=(tf.TensorShape([None, feats_dim]),\n",
    "                                               tf.TensorShape([None]),\n",
    "                                               tf.TensorShape([None]),\n",
    "                                               tf.TensorShape([]),\n",
    "                                               tf.TensorShape([])),\n",
    "                                 padding_values=(tf.cast(0, tf.float32),\n",
    "                                                tf.cast(labels_eos_id, tf.int64),\n",
    "                                                tf.cast(labels_eos_id, tf.int64),\n",
    "                                                tf.cast(0, tf.int64),\n",
    "                                                tf.cast(0, tf.int64)))\n",
    "        def key_func(features, labels_in, labels_out, feats_seq_len, labels_in_seq_len):\n",
    "            f0 = lambda: tf.constant(0, tf.int64)\n",
    "            f1 = lambda: tf.constant(1, tf.int64)\n",
    "            f2 = lambda: tf.constant(2, tf.int64)\n",
    "            f3 = lambda: tf.constant(3, tf.int64)\n",
    "            f4 = lambda: tf.constant(4, tf.int64)\n",
    "            f5 = lambda: tf.constant(5, tf.int64)\n",
    "            f6 = lambda: tf.constant(6, tf.int64)\n",
    "            \n",
    "            return tf.case([(tf.less_equal(feats_seq_len, 200), f0),\n",
    "                   (tf.less_equal(feats_seq_len, 250), f1),\n",
    "                   (tf.less_equal(feats_seq_len, 300), f2),\n",
    "                   (tf.less_equal(feats_seq_len, 350), f3),\n",
    "                   (tf.less_equal(feats_seq_len, 400), f4),\n",
    "                   (tf.less_equal(feats_seq_len, 500), f5)], default=f6)\n",
    "        \n",
    "        def reduce_func(bucket_id, windowed_data):\n",
    "            return batching_func(windowed_data)\n",
    "        \n",
    "        if model_type=='train':\n",
    "            dataset = dataset.shuffle(10000)\n",
    "            batched_dataset = dataset.group_by_window(key_func=key_func, reduce_func=reduce_func, window_size=batch_size)\n",
    "            batched_dataset = batched_dataset.shuffle(10000)\n",
    "        else:\n",
    "            batched_dataset = batching_func(dataset)\n",
    "            \n",
    "        return batched_dataset.make_initializable_iterator()\n",
    "    \n",
    "    def _compute_encoder_outputs(self, features, feats_seq_len, num_unit_encoder, n_hidden_layer):\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        self.training = tf.placeholder(tf.bool)\n",
    "        \n",
    "        def residual_block(inp, out_channels, strides=(1,1)):\n",
    "            inp_channels = inp.get_shape().as_list()[3]\n",
    "            \n",
    "            out = tf.layers.batch_normalization(inp, training=self.training) \n",
    "            out = tf.nn.relu(out)\n",
    "            out = tf.layers.conv2d(out, filters=out_channels, kernel_size=(3,3), strides=strides, padding='same')\n",
    "            out = tf.layers.dropout(out, rate=1-self.keep_prob, training=self.training)\n",
    "            out = tf.layers.batch_normalization(out, training=self.training) \n",
    "            out = tf.nn.relu(out)\n",
    "            out = tf.layers.conv2d(out, filters=out_channels, kernel_size=(3,3), strides=(1,1), padding='same')\n",
    "            out = tf.layers.dropout(out, rate=1-self.keep_prob, training=self.training)\n",
    "            \n",
    "            if strides != (1,1): # down-sample\n",
    "                inp = tf.layers.max_pooling2d(inp, pool_size=strides, strides=strides, padding='same')\n",
    "            if inp_channels != out_channels:\n",
    "                inp = tf.layers.conv2d(inp, filters=out_channels, kernel_size=(1,1), strides=(1,1), padding='same')\n",
    "\n",
    "            return out + inp\n",
    "        \n",
    "        features = tf.stack(tf.split(features, num_or_size_splits=3, axis=2), axis=3)\n",
    "        features = tf.transpose(features, [0,2,1,3]) # shape = [batch, feats_dim/3, max_time, channels]\n",
    "        \n",
    "        conv = tf.layers.conv2d(features, filters=32, kernel_size=(3,3), strides=(3,3), padding='same',\n",
    "                                activation=tf.nn.relu) # 41 -> 14, time -> time/3\n",
    "        res1 = residual_block(conv, 32, strides=(1,1)) # \n",
    "        res2 = residual_block(res1, 32, strides=(1,1))\n",
    "        res3 = residual_block(res2, 64, strides=(1,1)) # \n",
    "        res4 = residual_block(res3, 64, strides=(1,1))\n",
    "        res5 = residual_block(res4, 64, strides=(1,1)) # \n",
    "        #res6 = residual_block(res5, 64, strides=(1,1)) \n",
    "        #pool = tf.contrib.layers.avg_pool2d(res3_2, kernel_size=(3,3), stride=(3,3), padding='SAME')\n",
    "        \n",
    "        flattend = tf.transpose(res5, [0,2,1,3])\n",
    "        flattend = tf.reshape(flattend, [tf.shape(flattend)[0], tf.shape(flattend)[1], 14*64])\n",
    "        fc = tf.layers.dense(flattend, 256)\n",
    "        fc = tf.layers.batch_normalization(fc, training=self.training) \n",
    "        fc = tf.nn.relu(fc)\n",
    "        #fc = tf.layers.dropout(fc, rate=1-self.keep_prob, training=self.training)\n",
    "        \n",
    "        cell_fw = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_unit_encoder, use_peepholes=True), output_keep_prob=self.keep_prob)\n",
    "        cell_bw = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_unit_encoder, use_peepholes=True), output_keep_prob=self.keep_prob)\n",
    "        outputs, (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, fc, sequence_length=feats_seq_len//3, dtype=tf.float32)\n",
    "        outputs = tf.concat(outputs, 2)\n",
    "        return outputs, feats_seq_len//3\n",
    "    \n",
    "    def _get_decoder_cell_and_init_state(self, mem_seq_len, num_unit_decoder, n_hidden_layer, memory, beam_width, model_type):\n",
    "        \n",
    "        batch_size = tf.shape(memory)[0]\n",
    "        #decoder_cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.GRUCell(num_unit_decoder),\n",
    "        #                                                            output_keep_prob=self.keep_prob) for _ in range(2)])\n",
    "        decoder_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_unit_decoder, use_peepholes=True), output_keep_prob=self.keep_prob)\n",
    "        #decoder_cell = tf.nn.rnn_cell.LSTMCell(num_unit_decoder)\n",
    "\n",
    "        if model_type == 'infer':\n",
    "            memory = tf.contrib.seq2seq.tile_batch(memory, multiplier=beam_width)\n",
    "            mem_seq_len = tf.contrib.seq2seq.tile_batch(mem_seq_len, multiplier=beam_width)\n",
    "            batch_size = batch_size * beam_width\n",
    "\n",
    "        attention_mechanism = tf.contrib.seq2seq.LuongAttention(num_unit_decoder, memory, mem_seq_len, scale=False)\n",
    "        decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism, attention_layer_size=num_unit_decoder)\n",
    "        decoder_initial_state = decoder_cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        return(decoder_cell, decoder_initial_state)\n",
    "    \n",
    "    def _compute_dynamic_decode(self, features, labels_in, feats_seq_len, labels_in_seq_len, num_unit_encoder, num_unit_decoder,\n",
    "                        n_hidden_layer, num_classes, beam_width, model_type):\n",
    "        embedding_decoder = tf.Variable(np.identity(num_classes), dtype=tf.float32, trainable=False)\n",
    "        decoder_emb_inp = tf.nn.embedding_lookup(embedding_decoder, labels_in)\n",
    "    \n",
    "        memory, mem_seq_len = self._compute_encoder_outputs(features, feats_seq_len, num_unit_encoder, n_hidden_layer)\n",
    "        with tf.variable_scope('decoder') as decoder_scope:\n",
    "            decoder_cell, decoder_initial_state = self._get_decoder_cell_and_init_state(mem_seq_len, num_unit_decoder, n_hidden_layer,\n",
    "                                                                                        memory, beam_width, model_type)\n",
    "        \n",
    "            output_layer = tf.contrib.keras.layers.Dense(num_classes, use_bias=False)\n",
    "\n",
    "            if model_type in ['train', 'eval']:\n",
    "                helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inp, labels_in_seq_len, time_major=False)\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state=decoder_initial_state)\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=False, scope=decoder_scope)\n",
    "                return output_layer(outputs.rnn_output)\n",
    "            else: # model_type == 'infer'\n",
    "                beam_decoder = tf.contrib.seq2seq.BeamSearchDecoder(decoder_cell, embedding_decoder, \n",
    "                                                    tf.fill([tf.shape(features)[0]], labels_sos_id), labels_eos_id,\n",
    "                                                            decoder_initial_state, beam_width, output_layer=output_layer)\n",
    "                decoded, _, final_seq_len = tf.contrib.seq2seq.dynamic_decode(beam_decoder, maximum_iterations=100,\n",
    "                                                                              output_time_major=False, scope=decoder_scope)\n",
    "                return decoded.predicted_ids # shape = [batch, max_time, beam_width]\n",
    "    \n",
    "    def _compute_loss(self, labels_out, labels_in_seq_len, logits):\n",
    "        max_time = tf.shape(labels_out)[1]\n",
    "        target_weights = tf.sequence_mask(labels_in_seq_len, max_time, dtype=logits.dtype)\n",
    "        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_out, logits=logits)\n",
    "        return tf.reduce_sum(crossent * target_weights) / tf.to_float(tf.shape(logits)[0])\n",
    "    \n",
    "    def _get_update_step(self, loss, learning_rate, optimizer):\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            params = tf.trainable_variables()\n",
    "            gradients = tf.gradients(loss, params)\n",
    "            clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "            if optimizer == 'adam':\n",
    "                opt = tf.train.AdamOptimizer(learning_rate)\n",
    "            elif optimizer == 'sgd':\n",
    "                opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "            elif optimizer =='momentum':\n",
    "                opt = tf.train.MomentumOptimizer(learning_rate, momentum=0.9, use_nesterov=True)\n",
    "            update_step = opt.apply_gradients(zip(clipped_gradients, params))\n",
    "        return update_step\n",
    "    \n",
    "    def _compute_per(self, predicted_ids, labels_out, labels_eos_id, phn_61, phn_39, mapping):\n",
    "        \n",
    "        phn_61_tensor = tf.constant(phn_61, dtype=tf.string)\n",
    "        phn_39_tensor = tf.constant(phn_39, dtype=tf.string)\n",
    "        mapping_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(list(mapping.keys()), list(mapping.values())), default_value='')\n",
    "        self.mapping_table_init = mapping_table.init\n",
    "        \n",
    "        def map_to_reduced_phn(p):\n",
    "            val = mapping_table.lookup(phn_61_tensor[p])\n",
    "            f1 = lambda: tf.to_int32(tf.reduce_min(tf.where(tf.equal(val, phn_39_tensor))))\n",
    "            f2 = lambda: tf.to_int32(tf.reduce_min(tf.where(tf.equal(phn_61_tensor[p], phn_39_tensor))))\n",
    "            return tf.cond(tf.not_equal(val, ''), f1, f2)\n",
    "        \n",
    "        indices = tf.to_int64(tf.where(tf.logical_and(tf.not_equal(predicted_ids[:,:,0], -1), tf.not_equal(predicted_ids[:,:,0], labels_eos_id))))\n",
    "        vals = tf.to_int32(tf.gather_nd(predicted_ids[:,:,0], indices))\n",
    "        shape = tf.to_int64(tf.shape(predicted_ids[:,:,0]))\n",
    "        decoded_sparse = tf.SparseTensor(indices, vals, shape)\n",
    "        labels_out_sparse = self._get_sparse_tensor(labels_out, labels_eos_id)\n",
    "        \n",
    "        decoded_reduced = tf.SparseTensor(decoded_sparse.indices, tf.map_fn(map_to_reduced_phn, decoded_sparse.values), decoded_sparse.dense_shape)\n",
    "        labels_out_reduced = tf.SparseTensor(labels_out_sparse.indices, tf.map_fn(map_to_reduced_phn, labels_out_sparse.values), labels_out_sparse.dense_shape)\n",
    "        \n",
    "        return tf.reduce_sum(tf.edit_distance(decoded_reduced, labels_out_reduced, normalize=False)) , tf.to_float(tf.size(labels_out_reduced.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, \n",
      "train_loss=144.059, time=66s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-1\n",
      "\tdev_loss=119.246, time=5s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-1\n",
      "\t\ttest_per=0.741, time=11s\n",
      "Epoch 2/100, \n",
      "train_loss=111.972, time=59s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-2\n",
      "\tdev_loss=108.869, time=4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-2\n",
      "\t\ttest_per=0.731, time=7s\n",
      "Epoch 3/100, \n",
      "train_loss=100.981, time=61s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-3\n",
      "\tdev_loss=94.834, time=4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-3\n",
      "\t\ttest_per=0.679, time=9s\n",
      "Epoch 4/100, \n",
      "train_loss=83.774, time=58s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-4\n",
      "\tdev_loss=82.152, time=4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-4\n",
      "\t\ttest_per=0.651, time=10s\n",
      "Epoch 5/100, \n",
      "train_loss=67.897, time=60s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-5\n",
      "\tdev_loss=58.039, time=4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-5\n",
      "\t\ttest_per=0.482, time=10s\n",
      "Epoch 6/100, \n",
      "train_loss=57.553, time=58s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-6\n",
      "\tdev_loss=45.546, time=4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-6\n",
      "\t\ttest_per=0.396, time=10s\n",
      "Epoch 7/100, \n",
      "train_loss=50.464, time=59s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-7\n",
      "\tdev_loss=43.521, time=4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-7\n",
      "\t\ttest_per=0.363, time=11s\n",
      "Epoch 8/100, \n",
      "train_loss=45.658, time=59s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-8\n",
      "\tdev_loss=39.380, time=4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-8\n",
      "\t\ttest_per=0.342, time=11s\n",
      "Epoch 9/100, \n",
      "train_loss=41.843, time=59s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-9\n",
      "\tdev_loss=36.120, time=4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-9\n",
      "\t\ttest_per=0.287, time=10s\n",
      "Epoch 10/100, \n",
      "train_loss=38.712, time=58s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-10\n",
      "\tdev_loss=33.847, time=4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-10\n",
      "\t\ttest_per=0.287, time=11s\n",
      "Epoch 11/100, \n",
      "train_loss=36.324, time=59s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-11\n",
      "\tdev_loss=31.937, time=4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-11\n",
      "\t\ttest_per=0.271, time=10s\n",
      "Epoch 12/100, \n",
      "train_loss=34.284, time=56s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-12\n",
      "\tdev_loss=31.533, time=3s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-12\n",
      "\t\ttest_per=0.263, time=10s\n",
      "Epoch 13/100, \n",
      "train_loss=32.823, time=57s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-13\n",
      "\tdev_loss=30.966, time=3s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-13\n",
      "\t\ttest_per=0.264, time=10s\n",
      "Epoch 14/100, \n",
      "train_loss=31.141, time=56s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-14\n",
      "\tdev_loss=29.907, time=4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-14\n",
      "\t\ttest_per=0.240, time=10s\n",
      "Epoch 15/100, \n",
      "train_loss=29.769, time=57s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-15\n",
      "\tdev_loss=28.846, time=4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-15\n",
      "\t\ttest_per=0.233, time=10s\n",
      "Epoch 16/100, \n",
      "train_loss=28.414, time=57s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-16\n",
      "\tdev_loss=28.662, time=3s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-16\n",
      "\t\ttest_per=0.243, time=10s\n",
      "Epoch 17/100, \n",
      "train_loss=27.244, time=57s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-17\n",
      "\tdev_loss=27.859, time=3s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-17\n",
      "\t\ttest_per=0.235, time=10s\n",
      "Epoch 18/100, \n",
      "train_loss=26.185, time=56s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-18\n",
      "\tdev_loss=27.376, time=4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-18\n",
      "\t\ttest_per=0.221, time=10s\n",
      "Epoch 19/100, \n",
      "train_loss=25.021, time=57s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-19\n",
      "\tdev_loss=27.359, time=3s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-19\n",
      "\t\ttest_per=0.220, time=10s\n",
      "Epoch 20/100, \n",
      "train_loss=24.182, time=57s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-20\n",
      "\tdev_loss=27.609, time=4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-20\n",
      "\t\ttest_per=0.220, time=10s\n",
      "Epoch 21/100, \n",
      "train_loss=23.268, time=56s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-21\n",
      "\tdev_loss=26.463, time=4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-21\n",
      "\t\ttest_per=0.215, time=10s\n",
      "Epoch 22/100, \n",
      "train_loss=22.541, time=56s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-22\n",
      "\tdev_loss=27.696, time=4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-22\n",
      "\t\ttest_per=0.225, time=10s\n",
      "Epoch 23/100, \n",
      "train_loss=21.776, time=57s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-23\n",
      "\tdev_loss=26.343, time=4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-23\n",
      "\t\ttest_per=0.221, time=10s\n",
      "Epoch 24/100, \n",
      "train_loss=21.168, time=57s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-24\n",
      "\tdev_loss=26.224, time=4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-24\n",
      "\t\ttest_per=0.219, time=10s\n",
      "Epoch 25/100, \n",
      "train_loss=20.590, time=56s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-25\n",
      "\tdev_loss=27.026, time=4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-25\n",
      "\t\ttest_per=0.222, time=10s\n",
      "Epoch 26/100, \n",
      "train_loss=20.079, time=56s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOutOfRangeError\u001b[0m: End of sequence\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,123], [?,?], [?,?], [?], [?]], output_types=[DT_FLOAT, DT_INT64, DT_INT64, DT_INT64, DT_INT64], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Iterator)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7a845d4c9d6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[1;31m#train_model.update_step.run(session=train_sess, feed_dict={train_model.keep_prob: 1.0})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1339\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1340\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOutOfRangeError\u001b[0m: End of sequence\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,123], [?,?], [?,?], [?], [?]], output_types=[DT_FLOAT, DT_INT64, DT_INT64, DT_INT64, DT_INT64], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Iterator)]]\n\nCaused by op 'IteratorGetNext', defined at:\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-7a845d4c9d6e>\", line 7, in <module>\n    learning_rate, optimizer='adam', file_type=TRAIN_FILE, model_type='train')\n  File \"<ipython-input-3-46ab5f245b4c>\", line 7, in __init__\n    batched_data = iterator.get_next()\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\data\\python\\ops\\dataset_ops.py\", line 248, in get_next\n    name=name))\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\", line 303, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2583, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"c:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nOutOfRangeError (see above for traceback): End of sequence\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,123], [?,?], [?,?], [?], [?]], output_types=[DT_FLOAT, DT_INT64, DT_INT64, DT_INT64, DT_INT64], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Iterator)]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'T'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7a845d4c9d6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Epoch {}/{}, \\ntrain_loss={:.3f}, time={:.0f}s\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mcheckpoint_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoints_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mdev_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_sess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[0;32m   1494\u001b[0m           checkpoint_file, meta_graph_suffix=meta_graph_suffix)\n\u001b[0;32m   1495\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1496\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport_meta_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_graph_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1498\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[1;34m(self, filename, collection_list, as_text, export_scope, clear_devices, clear_extraneous_savers)\u001b[0m\n\u001b[0;32m   1532\u001b[0m         \u001b[0mexport_scope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexport_scope\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1533\u001b[0m         \u001b[0mclear_devices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1534\u001b[1;33m         clear_extraneous_savers=clear_extraneous_savers)\n\u001b[0m\u001b[0;32m   1535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1536\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[1;34m(filename, meta_info_def, graph_def, saver_def, collection_list, as_text, graph, export_scope, clear_devices, clear_extraneous_savers, **kwargs)\u001b[0m\n\u001b[0;32m   1765\u001b[0m       \u001b[0mclear_devices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1766\u001b[0m       \u001b[0mclear_extraneous_savers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclear_extraneous_savers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1767\u001b[1;33m       **kwargs)\n\u001b[0m\u001b[0;32m   1768\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\meta_graph.py\u001b[0m in \u001b[0;36mexport_scoped_meta_graph\u001b[1;34m(filename, graph_def, graph, export_scope, as_text, unbound_inputs_col_name, clear_devices, saver_def, clear_extraneous_savers, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m       \u001b[0mclear_extraneous_savers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclear_extraneous_savers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    828\u001b[0m       \u001b[0msaver_def\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 829\u001b[1;33m       **kwargs)\n\u001b[0m\u001b[0;32m    830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    831\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\meta_graph.py\u001b[0m in \u001b[0;36mcreate_meta_graph_def\u001b[1;34m(meta_info_def, graph_def, saver_def, collection_list, graph, export_scope, exclude_nodes, clear_extraneous_savers)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_graph_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m     \u001b[0mmeta_graph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m   \u001b[1;31m# Fills in meta_info_def.stripped_op_list using the ops from graph_def.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36mMergeFrom\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m   1221\u001b[0m           \u001b[0mfield_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m           \u001b[0mfields\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1223\u001b[1;33m         \u001b[0mfield_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1224\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpp_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mCPPTYPE_MESSAGE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1225\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_present_in_parent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\u001b[0m in \u001b[0;36mMergeFrom\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    395\u001b[0m     \u001b[0mone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopying\u001b[0m \u001b[0meach\u001b[0m \u001b[0mindividual\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \"\"\"\n\u001b[1;32m--> 397\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\u001b[0m in \u001b[0;36mextend\u001b[1;34m(self, elem_seq)\u001b[0m\n\u001b[0;32m    387\u001b[0m       \u001b[0mnew_element\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmessage_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m       \u001b[0mnew_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SetListener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlistener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m       \u001b[0mnew_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m       \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[0mlistener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModified\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36mMergeFrom\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m   1221\u001b[0m           \u001b[0mfield_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m           \u001b[0mfields\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1223\u001b[1;33m         \u001b[0mfield_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1224\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpp_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mCPPTYPE_MESSAGE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1225\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_present_in_parent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\u001b[0m in \u001b[0;36mMergeFrom\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;31m# self._message_listener.Modified() not required here, because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[1;31m# mutations to submessages already propagate.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    555\u001b[0m       \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_key_checker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCheckValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m       \u001b[0mnew_element\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_message_descriptor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_concrete_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m       \u001b[0mnew_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SetListener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_message_listener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_element\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_message_listener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModified\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_graph = tf.Graph()\n",
    "dev_graph = tf.Graph()\n",
    "test_graph = tf.Graph()\n",
    "\n",
    "with train_graph.as_default():\n",
    "    train_model = Model(batch_size, num_unit_encoder, num_unit_decoder, n_hidden_layer, feats_dim, num_classes, labels_sos_id, labels_eos_id,\n",
    "                 learning_rate, optimizer='adam', file_type=TRAIN_FILE, model_type='train')\n",
    "    initializer = tf.global_variables_initializer()\n",
    "\n",
    "with dev_graph.as_default():\n",
    "    dev_model = Model(batch_size, num_unit_encoder, num_unit_decoder, n_hidden_layer, feats_dim, num_classes, labels_sos_id, labels_eos_id,\n",
    "                 file_type=DEV_FILE, model_type='eval')\n",
    "with test_graph.as_default():\n",
    "    test_model = Model(batch_size, num_unit_encoder, num_unit_decoder, n_hidden_layer, feats_dim, num_classes, labels_sos_id, labels_eos_id,\n",
    "                 beam_width=10, phn_61=phn_61, phn_39=phn_39, mapping=mapping, file_type=TEST_FILE, model_type='infer')\n",
    "    \n",
    "train_sess = tf.Session(graph=train_graph)\n",
    "train_sess.run(initializer)\n",
    "\n",
    "dev_sess = tf.Session(graph=dev_graph)\n",
    "\n",
    "test_sess = tf.Session(graph=test_graph)\n",
    "test_sess.run(test_model.mapping_table_init)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_sess.run(train_model.iterator_initializer)\n",
    "    train_loss = []\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        try:\n",
    "            _, cost = train_sess.run([train_model.update_step, train_model.loss], feed_dict={train_model.keep_prob: 0.7, train_model.training: True})\n",
    "            #train_model.update_step.run(session=train_sess, feed_dict={train_model.keep_prob: 1.0})\n",
    "            train_loss.append(cost)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            end = time.time()\n",
    "            log = \"Epoch {}/{}, \\ntrain_loss={:.3f}, time={:.0f}s\"\n",
    "            print(log.format(epoch+1, epochs, np.mean(train_loss), end-start))\n",
    "            checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=epoch+1)\n",
    "            \n",
    "            dev_model.saver.restore(dev_sess, checkpoint_path)\n",
    "            dev_sess.run(dev_model.iterator_initializer)\n",
    "            dev_loss = []\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                try:\n",
    "                    cost = dev_sess.run(dev_model.loss, feed_dict={dev_model.keep_prob: 1.0, dev_model.training: False})\n",
    "                    dev_loss.append(cost)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    end = time.time()\n",
    "                    log = \"\\tdev_loss={:.3f}, time={:.0f}s\"\n",
    "                    print(log.format(np.mean(dev_loss), end-start))\n",
    "                    break\n",
    "            \n",
    "            test_model.saver.restore(test_sess, checkpoint_path)\n",
    "            test_sess.run(test_model.iterator_initializer)\n",
    "            test_per = []\n",
    "            test_seq_len = []\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                try:\n",
    "                    _per, seq_len = test_sess.run(test_model.per, feed_dict={test_model.keep_prob: 1.0, test_model.training: False})\n",
    "                    test_per.append(_per)\n",
    "                    test_seq_len.append(seq_len)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    end = time.time()\n",
    "                    log = '\\t\\ttest_per={:.3f}, time={:.0f}s'\n",
    "                    print(log.format(sum(test_per)/sum(test_seq_len), end-start))\n",
    "                    break\n",
    "            \n",
    "            break\n",
    "            \n",
    "train_sess.close()\n",
    "dev_sess.close()\n",
    "test_sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "restored_ckpt_path = './model/seq2seq_residual/ckpt-24'\n",
    "ft_learning_rate = 0.003\n",
    "ft_batch_size = 32\n",
    "ft_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/ckpt-24\n",
      "Epoch 1/100: \n",
      "train_loss=18.497, time = 57s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/finetunning/ckpt-1\n",
      "\tdev_loss=24.954, time = 4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/finetunning/ckpt-1\n",
      "\t\ttest_per=0.200, time=10s\n",
      "Epoch 2/100: \n",
      "train_loss=17.577, time = 57s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/finetunning/ckpt-2\n",
      "\tdev_loss=25.276, time = 4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/finetunning/ckpt-2\n",
      "\t\ttest_per=0.202, time=10s\n",
      "Epoch 3/100: \n",
      "train_loss=17.221, time = 58s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/finetunning/ckpt-3\n",
      "\tdev_loss=25.423, time = 4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/finetunning/ckpt-3\n",
      "\t\ttest_per=0.207, time=10s\n",
      "Epoch 4/100: \n",
      "train_loss=16.923, time = 57s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/finetunning/ckpt-4\n",
      "\tdev_loss=25.277, time = 4s\n",
      "INFO:tensorflow:Restoring parameters from ./model/seq2seq_residual/finetunning/ckpt-4\n",
      "\t\ttest_per=0.201, time=10s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2a4bca5b6378>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             _, cost = ft_train_sess.run([ft_train_model.update_step, ft_train_model.loss], feed_dict={ft_train_model.keep_prob: 0.7,\n\u001b[1;32m---> 34\u001b[1;33m                                                                                                      ft_train_model.training: True})\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[0mft_train_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sappyprg\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ft_train_graph = tf.Graph()\n",
    "ft_dev_graph = tf.Graph()\n",
    "ft_test_graph = tf.Graph()\n",
    "\n",
    "with ft_train_graph.as_default():\n",
    "    ft_train_model = Model(ft_batch_size, num_unit_encoder, num_unit_decoder, n_hidden_layer, feats_dim, num_classes, labels_sos_id,\n",
    "                           labels_eos_id, ft_learning_rate, optimizer='momentum', file_type=TRAIN_FILE, model_type='train')\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    \n",
    "with ft_dev_graph.as_default():\n",
    "    ft_dev_model = Model(ft_batch_size, num_unit_encoder, num_unit_decoder, n_hidden_layer, feats_dim, num_classes, labels_sos_id, \n",
    "                         labels_eos_id, file_type=DEV_FILE, model_type='eval')\n",
    "\n",
    "with ft_test_graph.as_default():\n",
    "    ft_test_model = Model(ft_batch_size, num_unit_encoder, num_unit_decoder, n_hidden_layer, feats_dim, num_classes, labels_sos_id,\n",
    "                          labels_eos_id, beam_width=10, phn_61=phn_61, phn_39=phn_39, mapping=mapping, file_type=TEST_FILE, model_type='infer')    \n",
    "    \n",
    "\n",
    "ft_train_sess = tf.Session(graph=ft_train_graph)\n",
    "ft_train_sess.run(initializer)\n",
    "ft_dev_sess = tf.Session(graph=ft_dev_graph)\n",
    "ft_test_sess = tf.Session(graph=ft_test_graph)\n",
    "ft_test_sess.run(ft_test_model.mapping_table_init)\n",
    "\n",
    "ft_train_model.saver.restore(ft_train_sess, restored_ckpt_path)\n",
    "\n",
    "for epoch in range(ft_epochs):\n",
    "    ft_train_sess.run(ft_train_model.iterator_initializer)\n",
    "    ft_train_loss  = []\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        try:\n",
    "            _, cost = ft_train_sess.run([ft_train_model.update_step, ft_train_model.loss], feed_dict={ft_train_model.keep_prob: 0.7,\n",
    "                                                                                                     ft_train_model.training: True})\n",
    "            ft_train_loss.append(cost)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            end = time.time()\n",
    "            log = \"Epoch {}/{}: \\ntrain_loss={:.3f}, time = {:.0f}s\"\n",
    "            print(log.format(epoch+1, ft_epochs, np.mean(ft_train_loss), end-start))\n",
    "            ft_checkpoint_path = ft_train_model.saver.save(ft_train_sess, ft_checkpoints_path, global_step=epoch+1)\n",
    "            \n",
    "            ft_dev_model.saver.restore(ft_dev_sess, ft_checkpoint_path)\n",
    "            ft_dev_sess.run(ft_dev_model.iterator_initializer)\n",
    "            ft_dev_loss = []\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                try:\n",
    "                    cost = ft_dev_sess.run(ft_dev_model.loss, feed_dict={ft_dev_model.keep_prob: 1.0, ft_dev_model.training: False})\n",
    "                    ft_dev_loss.append(cost)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    end = time.time()\n",
    "                    log = \"\\tdev_loss={:.3f}, time = {:.0f}s\"\n",
    "                    print(log.format(np.mean(ft_dev_loss), end-start))\n",
    "                    break\n",
    "                \n",
    "            ft_test_model.saver.restore(ft_test_sess, ft_checkpoint_path)\n",
    "            ft_test_sess.run(ft_test_model.iterator_initializer)\n",
    "            ft_test_per = []\n",
    "            ft_test_seq_len = []\n",
    "            start = time.time()\n",
    "            while True:\n",
    "                try:\n",
    "                    _per, seq_len = ft_test_sess.run(ft_test_model.per, feed_dict={ft_test_model.keep_prob: 1.0, ft_test_model.training: False})\n",
    "                    ft_test_per.append(_per)\n",
    "                    ft_test_seq_len.append(seq_len)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    end = time.time()\n",
    "                    log = '\\t\\ttest_per={:.3f}, time={:.0f}s'\n",
    "                    print(log.format(sum(ft_test_per)/sum(ft_test_seq_len), end-start))\n",
    "                    break\n",
    "            # go to netxt epoch\n",
    "            break\n",
    "        \n",
    "        \n",
    "ft_train_sess.close()\n",
    "ft_dev_sess.close()\n",
    "ft_test_sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "restored_ckpt_path = './model/seq2seq_residual/finetunning/ckpt-10'\n",
    "ft_learning_rate = 0.001\n",
    "ft_batch_size = 32\n",
    "ft_epochs = 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
